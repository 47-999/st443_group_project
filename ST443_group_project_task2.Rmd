---
title: "ST443 Group Project - Task 2"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

# ST443 Group Project

# Task 2: Feature selection

## Introduction

### Data Preparation and split into Train and Test Data

```{r}
library(ggplot2)
library(dplyr)
library(caret)
library(glmnet)
library(pROC)
```

```{r}
MLData_Task2 <- read.csv("data2.csv.gz", header=TRUE)
View(MLData_Task2)
```

```{r}
any(is.na(MLData_Task2))
```

```{r}
# Random split (80% training, 20% testing)
set.seed(123)
train_indices <- sample(1:nrow(MLData_Task2), size = 0.8 * nrow(MLData_Task2))
```

```{r}
# Create training and testing datasets
train2_data <- MLData_Task2[train_indices, ]
test2_data <- MLData_Task2[-train_indices, ]

# Prepare data
train_X <- as.matrix(train2_data[, -1])  # Exclude label column
test_X <- as.matrix(test2_data[, -1])  # Exclude label column
train_Y <- factor(train2_data$label, levels = c(-1, 1), labels = c(0, 1))  # Convert to 0 and 1
test_Y <- factor(test2_data$label, levels = c(-1, 1), labels = c(0, 1))  # Convert to 0 and 1

# Verify split
cat("Training set size:", nrow(train2_data), "\n")
cat("Test set size:", nrow(test2_data), "\n")
```

```{r}
# Count the number of each class to see whether the dataset is balanced
table(train_Y)
table(test_Y)
```

We can see that the data is heavily imbalanced - it contains much more observations of class 0 than of class 1.

### Lasso with Logistic Regression

At first, we use cross-validation on the full feature set to determine the best penalty coefficient lambda for the dataset.

```{r}
# Train Lasso Regression with cross-validation
set.seed(123)
lasso_cv_full <- cv.glmnet(train_X, train_Y, alpha = 1, family = "binomial")
```

```{r}
# Best lambda from cross-validation (the one with the lowest cross-validation error)
# For comment: The larger the lambda, the more coefficients have been sunk to zero
best_lambda <- lasso_cv_full$lambda.min
cat("Best lambda selected:", best_lambda, "\n")
```

As we realised that our dataset is heavily imbalanced, i.e. there are much more observations of class 0 than class 1, we take that into account when fitting our lasso model with the selected best lambda.

```{r}
# Calculate weights
n_class0 <- 580  # Number of samples in class 0
n_class1 <- 60   # Number of samples in class 1
n_total <- n_class0 + n_class1

weight_class0 <- n_total / n_class0
weight_class1 <- n_total / n_class1

# Assign weights to each observation
weights <- ifelse(train_Y == 0, weight_class0, weight_class1)
lasso_model <- glmnet(train_X, train_Y, alpha = 1, family = "binomial", lambda = best_lambda, weights = weights)
```

Predict the probabilities on the test data

```{r}
# Predict probabilities for the positive class
lasso_probs <- predict(lasso_model, newx= test_X, s= best_lambda, type = "response")
# Ensure that the outout is a numeric vector
lasso_probs <- as.numeric(lasso_probs)
```

Now we convert the predicted probabilities into the class labels at the optimal threshold.

```{r}
# Convert probabilities to class labels at optimal threshold
lasso_predictions <- ifelse(lasso_probs > 0.01, 0, 1) 
lasso_predictions <- as.factor(lasso_predictions)
```

Assessment of our model:

```{r}
# Compute confusion matrix
conf_matrix_lasso <- confusionMatrix(lasso_predictions, test_Y)
print(conf_matrix_lasso)

# Extract the confusion matrix table
conf_matrix_table <- conf_matrix_lasso$table
conf_matrix_table

# True positives and false negatives
TP <- sum(lasso_predictions == 0 & test_Y == 0)
FP <- sum(lasso_predictions == 0 & test_Y == 1)
TN <- sum(lasso_predictions == 1 & test_Y == 1)
FN <- sum(lasso_predictions == 1 & test_Y == 0)
```

```{r}
#calculating Sensitivity (Recall) for Positive and Negative Classes
sensitivity_positive <- ifelse((TP + FN) == 0, NA, TP / (TP + FN))  # Sensitivity for Positive class (0)
sensitivity_negative <- ifelse((TN + FP) == 0, NA, TN / (TN + FP))  # Sensitivity for Negative class (1)
```

```{r}
# Handling NaN or NA values in sensitivities
sensitivity_positive <- ifelse(is.na(sensitivity_positive), 0, sensitivity_positive)
sensitivity_negative <- ifelse(is.na(sensitivity_negative), 0, sensitivity_negative)
```

```{r}
# Calculating Balanced Accuracy
balanced_accuracy <- (sensitivity_positive + sensitivity_negative) / 2
cat("Balanced Accuracy:", balanced_accuracy, "\n")
```

#### Random Forest for Feature Selection

```{r}
library(randomForest)
library(caret)
```

For a random forest of classification trees, we usually use a random selection of m= sqrt(p) predictors as split candidates each time a split in a tree is considered. However in our case, this would mean m = sqrt(100,000) = \~316, which is way to computationally expensive. Thus we will apply a pre-selection of features by removing those features with a variance below 0.01 - given the low variance these features will in no case serve as good predictors for our classification task.

```{r}
# Calculate the variance of each feature
feature_variances <- apply(MLData_Task2[, -1], 2, var)

# Identify features with variance >= 0.01
selected_features <- which(feature_variances >= 0.01)

# Subset the dataset to keep only selected features
filtered_dataset <- MLData_Task2[, selected_features]

# Print the number of features removed
cat("Number of features removed:", ncol(MLData_Task2) - length(selected_features), "\n")
cat("Number of features retained:", length(selected_features), "\n")
```

sqrt(30,121) still leaves \~173 features to be considered at each split. This is still too computationally expensive, thus we will try for 2, 5 and 10.

```{r}
# Define the hyperparameter grid for tuning
tune_grid <- expand.grid(
  mtry = c(2, 5, 10)  # Number of features considered at each split
)
```

```{r}
# Set up cross-validation
control <- trainControl(
  method = "cv",            # Cross-validation
  number = 5,               # 5-fold CV
  verboseIter = TRUE,       # Print progress
  savePredictions = "final" # Save predictions
)
```

```{r}
# Create new training and test data set with only selected features
train_X = train_x[, selected_features]

# Train the model
set.seed(42)
rf_tuned_model <- train(
  x = train_X,
  y = train_Y,
  method = "rf",
  metric = "Accuracy",      # Metric to optimize
  tuneGrid = tune_grid,     # Hyperparameter grid
  trControl = control,
  ntree = 200               # Number of trees
)

# Display the best parameters
print(rf_tuned_model$bestTune)
```

```{r}
# Predictting on test set
rf_predictions <- predict(rf_tuned_model, newdata = RFtest_X)
```

```{r}
# Confusion matrix
conf_matrix_rf <- confusionMatrix(rf_predictions, RFtest_Y)

# Display confusion matrix
print(conf_matrix_rf)
```

```{r}
# Calculate Balanced Accuracy
TP <- conf_matrix_rf$table[2, 2]  # True Positives
TN <- conf_matrix_rf$table[1, 1]  # True Negatives
FP <- conf_matrix_rf$table[1, 2]  # False Positives
FN <- conf_matrix_rf$table[2, 1]  # False Negatives

# Avoiding division by zero
sensitivity <- ifelse((TP + FN) > 0, TP / (TP + FN), 0)
specificity <- ifelse((TN + FP) > 0, TN / (TN + FP), 0)

# Calculating Balanced Accuracy
balanced_accuracy <- (sensitivity + specificity) / 2

cat("Balanced Accuracy:", balanced_accuracy, "\n")
```
