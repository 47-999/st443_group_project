---
title: "ST443 Group Project - Task 2"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

# ST443 Group Project

# Task 2: Feature selection

## Introduction

### Forward Stepwise Selection

```{r}
install.packages("glmnet")
```

```{r}
library(ggplot2)
library(dplyr)
library(caret)
library(glmnet)  # For Lasso Regression
```

```{r}
MLData_Task2 <- read.csv("C:\\Users\\Devisha\\OneDrive - London School of Economics\\Documents\\Machine Learning\\data2.csv.gz")
str(MLData_Task2)
```

```{r}
any(is.na(MLData_Task2))
```

```{r}
# Random split (80% training, 20% testing)
set.seed(123)
train_indices <- sample(1:nrow(MLData_Task2), size = 0.8 * nrow(MLData_Task2))
```

```{r}
# Create training and testing datasets
train2_data <- MLData_Task2[train_indices, ]
test2_data <- MLData_Task2[-train_indices, ]

# Verify split
cat("Training set size:", nrow(train2_data), "\n")
cat("Test set size:", nrow(test2_data), "\n")
```

### Lasso Regression

```{r}
# Train Lasso Regression with cross-validation
set.seed(123)
lasso_model <- cv.glmnet(as.matrix(train2_data[, -1]), train2_data$label, alpha = 1, family = "binomial")
```

```{r}
# Best lambda from cross-validation
best_lambda <- lasso_model$lambda.min
cat("Best lambda selected:", best_lambda, "\n")
```

```{r}
# Manually reduce regularization strength
adjusted_lambda <- best_lambda * 0.5  # Halve the penalty strength
selected_features <- which(coef(lasso_model, s = adjusted_lambda)[-1] != 0)
cat("Number of selected features:", length(selected_features), "\n")
```

```{r}
# Subset the training and test sets
train_X_lasso <- as.matrix(train2_data[, selected_features, drop = FALSE])
test_X_lasso <- as.matrix(test2_data[, selected_features, drop = FALSE])
train2_Y <- train2_data$label
test2_Y <- test2_data$label

# Verify dimensions
cat("Training set dimensions after feature selection:", dim(train_X_lasso), "\n")
cat("Test set dimensions after feature selection:", dim(test_X_lasso), "\n")
```

```{r}
# Refitting the Lasso model
set.seed(123)
lasso_model_selected <- glmnet(train_X_lasso, train2_Y, alpha = 1, family = "binomial")
```

```{r}
# Predict probabilities for the positive class
lasso_probs <- predict(lasso_model_selected, newx = test_X_lasso, s = best_lambda, type = "response")

```

```{r}
# Convert probabilities to class labels (threshold = 0.5)
lasso_predictions <- ifelse(lasso_probs > 0.5, 1, -1) 
lasso_predictions <- as.factor(lasso_predictions)
```

```{r}
# Align levels
test2_Y <- as.factor(test2_Y)
common_levels <- union(levels(lasso_predictions), levels(test2_Y))
lasso_predictions <- factor(lasso_predictions, levels = common_levels)
test2_Y <- factor(test2_Y, levels = common_levels)
```

```{r}
# Compute confusion matrix
conf_matrix_lasso <- confusionMatrix(lasso_predictions, test2_Y)
print(conf_matrix_lasso)

# Extract the confusion matrix table
conf_matrix_table <- conf_matrix_lasso$table

# Extract True Positives, True Negatives, False Positives, False Negatives
TP <- conf_matrix_table["-1", "-1"]  # True Positives for class -1
TN <- conf_matrix_table["1", "1"]    # True Negatives for class 1
FP <- conf_matrix_table["1", "-1"]   # False Positives
FN <- conf_matrix_table["-1", "1"]   # False Negatives

# Print extracted values
cat("True Positives (TP):", TP, "\n")
cat("True Negatives (TN):", TN, "\n")
cat("False Positives (FP):", FP, "\n")
cat("False Negatives (FN):", FN, "\n")
```

```{r}
#calculating Sensitivity (Recall) for Positive and Negative Classes
sensitivity_positive <- ifelse((TP + FN) == 0, NA, TP / (TP + FN))  # Sensitivity for Positive class (-1)
sensitivity_negative <- ifelse((TN + FP) == 0, NA, TN / (TN + FP))  # Sensitivity for Negative class (1)
```


```{r}
# Handling NaN or NA values in sensitivities
sensitivity_positive <- ifelse(is.na(sensitivity_positive), 0, sensitivity_positive)
sensitivity_negative <- ifelse(is.na(sensitivity_negative), 0, sensitivity_negative)
```


```{r}
# Calculating Balanced Accuracy
balanced_accuracy <- (sensitivity_positive + sensitivity_negative) / 2
cat("Balanced Accuracy:", balanced_accuracy, "\n")
```
#### Random Forest - Feature Selectionm 

```{r}
library(randomForest)
library(caret) # For confusion matrix and other metrics
```


```{r}
set.seed(42)  # For reproducibility

# Split into training and test sets (70-30 split)
RF_train_index <- createDataPartition(MLData_Task2$label, p = 0.7, list = FALSE)
RF_train_data <- MLData_Task2[RF_train_index, ]
RF_test_data <- MLData_Task2[-RF_train_index, ]

# Separate features and labels
RFtrain_X <- RF_train_data[, -ncol(RF_train_data)]  # Exclude label column
RFtrain_Y <- RF_train_data$label

RFtest_X <- RF_test_data[, -ncol(RF_test_data)]  # Exclude label column
RFtest_Y <- RF_test_data$label
```

```{r}
RFtrain_Y <- factor(RF_train_data$label)        # Convert label to factor for classification
RFtest_Y <- factor(RF_test_data$label)         # Convert label to factor for classification
```

```{r}
# Define the hyperparameter grid for tuning
tune_grid <- expand.grid(
  mtry = c(2, 5, 10)  # Number of features considered at each split
)
```

```{r}
# Set up cross-validation
control <- trainControl(
  method = "cv",            # Cross-validation
  number = 5,               # 5-fold CV
  verboseIter = TRUE,       # Print progress
  savePredictions = "final" # Save predictions
)
```

```{r}
# Train the model
set.seed(42)
rf_tuned_model <- train(
  x = RFtrain_X,
  y = RFtrain_Y,
  method = "rf",
  metric = "Accuracy",      # Metric to optimize
  tuneGrid = tune_grid,     # Hyperparameter grid
  trControl = control,
  ntree = 200               # Number of trees
)

# Display the best parameters
print(rf_tuned_model$bestTune)
```

```{r}
# Predictting on test set
rf_predictions <- predict(rf_tuned_model, newdata = RFtest_X)
```

```{r}
# Confusion matrix
conf_matrix_rf <- confusionMatrix(rf_predictions, RFtest_Y)

# Display confusion matrix
print(conf_matrix_rf)
```

```{r}
# Calculate Balanced Accuracy
TP <- conf_matrix_rf$table[2, 2]  # True Positives
TN <- conf_matrix_rf$table[1, 1]  # True Negatives
FP <- conf_matrix_rf$table[1, 2]  # False Positives
FN <- conf_matrix_rf$table[2, 1]  # False Negatives

# Avoiding division by zero
sensitivity <- ifelse((TP + FN) > 0, TP / (TP + FN), 0)
specificity <- ifelse((TN + FP) > 0, TN / (TN + FP), 0)

# Calculating Balanced Accuracy
balanced_accuracy <- (sensitivity + specificity) / 2

cat("Balanced Accuracy:", balanced_accuracy, "\n")
```

