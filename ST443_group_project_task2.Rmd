---
title: "ST443 Group Project - Task 2"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

# ST443 Group Project

# Task 2: Feature selection

## Introduction

The aim of task 2 is to select properties of a molecule in a compound or random probe determine whether a compound binds to a target site on thrombin. This knowledge is required to design new compounds that can be used in drugs.

## T1.1 Data Preparation and Summary Statistics

```{r}
# Load all libraries required to execute the code in this notebook
library(ggplot2)
library(dplyr)
library(caret)
library(glmnet) # To run lasso logistic regression
library(pROC) # To plot ROC curve
library(yardstick) # To calculate balanced accuracy
```

```{r}
# Load and view the data
MLData_Task2 <- read.csv("data2.csv.gz", header=TRUE)
View(MLData_Task2)
```

#### Dataset-level statistics

```{r}
# Check for any missing valus in the dataset
any(is.na(MLData_Task2))
```

```{r}
# Calculate the Feature-Row ratio
cat("Number of features:", ncol(MLData_Task2), "\n")
cat("Number of samples:", nrow(MLData_Task2), "\n")
cat("Feature-to-sample ratio:", ncol(MLData_Task2) / nrow(MLData_Task2), "\n")
```

```{r}
# Check if the dataset is balanced, i.e. if the frequency of each class is approx. the same
table(MLData_Task2$label)
```

We can see that the data is heavily imbalanced - it contains much more observations of class -1 than of class 1.

```{r}
# Calculate the overall sparsity of the dataset (proportion of zeros in the dataset)
sum(MLData_Task2 == 0) / (nrow(MLData_Task2) * ncol(MLData_Task2))
```

As we can see, the dataset is very sparse.

#### Feature-level Statistics

```{r}
# Variance-Based Feature Selection
feature_variances <- apply(MLData_Task2, 2, var)
cat("Proportion of low-variance features (< 0.01):", mean(feature_variances < 0.01), "\n")
```

## T2.2 Training and Evaluation of Feature Selection methods

### Split data into Train and Test data

```{r}
# Random split (80% training, 20% testing)
set.seed(123)
train_indices <- sample(1:nrow(MLData_Task2), size = 0.8 * nrow(MLData_Task2))
```

```{r}
# Create training and testing datasets
train2_data <- MLData_Task2[train_indices, ]
test2_data <- MLData_Task2[-train_indices, ]

# Prepare data
train_X <- as.matrix(train2_data[, -1])  # Exclude label column
test_X <- as.matrix(test2_data[, -1])  # Exclude label column
train_Y <- factor(train2_data$label, levels = c(-1, 1), labels = c(0, 1))  # Convert to 0 and 1
test_Y <- factor(test2_data$label, levels = c(-1, 1), labels = c(0, 1))  # Convert to 0 and 1

# Verify split
cat("Training set size:", nrow(train2_data), "\n")
cat("Test set size:", nrow(test2_data), "\n")
```

### Lasso with Logistic Regression

As we are dealing with an extremly high-dimensional dataset, we will perform a feature-pre-selection based on the variance of the features

```{r}
# Calculate the variance of each feature
feature_variances <- apply(train_X, 2, var)

# Identify features with variance >= 0.01
selected_features <- which(feature_variances >= 0.01)

# Print the number of features removed
cat("Number of features removed:", ncol(train_X) - length(selected_features), "\n")
cat("Number of features retained:", length(selected_features), "\n")

# Apply on training and test dataset
train_X <- train_X[, selected_features]
test_X <- test_X[, selected_features]
```

The performance metric that we are trying to optimize is the balanced accuracy. To be able to access this metric in our cross-validation when trying to find the best shrinkage parameter lambda, we store the balanced accuracy as a function.

```{r}
# Define custom balanced accuracy function
calculate_balanced_accuracy <- function(data, lev = NULL, model = NULL) {
  confusion_matrix <- table(data$obs, data$pred)
  
  # Handle edge cases
  if (nrow(confusion_matrix) < 2 || ncol(confusion_matrix) < 2) {
    sensitivity <- 0
    specificity <- 0
  } else {
    TP <- confusion_matrix[2, 2]
    FN <- confusion_matrix[2, 1]
    TN <- confusion_matrix[1, 1]
    FP <- confusion_matrix[1, 2]
    sensitivity <- TP / (TP + FN)  # True Positive Rate
    specificity <- TN / (TN + FP)  # True Negative Rate
  }
  
  balanced_acc <- (sensitivity + specificity) / 2
  return(c(BalancedAccuracy = balanced_acc))
}
```

Because of the high imbalance in classes in our dataset, we will assign weights according to the share of the different classes in the dataset and pass these to our model.

```{r}
train_Y <- factor(train_Y, levels = c(0, 1), labels = c("Class0", "Class1"))
class_weights <- ifelse(train_Y == "Class1",
                        1 / sum(train_Y == "Class1"),
                        1 / sum(train_Y == "Class0"))
```

We will perform a 5-fold cross validation on the data using the train control function and balanced accuracy as the metric for evaluation to find the best value for the penalty coefficient lambda.

```{r}
# Define train control with custom summaryFunction
train_control <- trainControl(
  method = "cv",                     # Cross-validation
  number = 10,                        # Number of folds
  summaryFunction = calculate_balanced_accuracy, # Use custom metric
  classProbs = TRUE,                 # Enable probability calculations
  savePredictions = "final"          # Save predictions for further analysis
)
```

```{r}
# Define grid for lambda values
lambda_grid <- expand.grid(
  alpha = 1,                          # LASSO (alpha = 1)
  lambda = 10^seq(-5, -0.5, length.out = 10) # Lambda grid
)
```

```{r}
# Train LASSO model
set.seed(42)
lasso_model <- train(
  x = train_X,
  y = train_Y,
  method = "glmnet",                  # Use glmnet for LASSO
  trControl = train_control,          # Custom train control
  tuneGrid = lambda_grid,             # Grid of lambdas
  metric = "BalancedAccuracy",        # Optimize balanced accuracy
  weights = class_weights
)
```

```{r}
# Print the best lambda
best_lambda <- lasso_model$bestTune$lambda
print(lasso_model$bestTune)
```

```{r}
# View results
print(lasso_model$results)

# Plot results
plot(lasso_model)
```

Predict the probabilities on the test data

```{r}
# Predict probabilities for the test set
lasso_probs <- predict(lasso_model, newdata = test_X, type = "prob", s=best_lambda)

# Extract probabilities for the positive class
lasso_probs <- lasso_probs[, "Class1"]
```

Now we convert the predicted probabilities into the class labels at the optimal threshold.

```{r}
# Convert probabilities to class labels at optimal threshold
lasso_predictions <- ifelse(lasso_probs > best_lambda, 0, 1) 
lasso_predictions <- factor(lasso_predictions, levels = levels(test_Y))
```

Assessment of our model:

```{r}
# Compute confusion matrix
conf_matrix_lasso <- confusionMatrix(lasso_predictions, test_Y)
print(conf_matrix_lasso)

# Extract the confusion matrix table
conf_matrix_table <- conf_matrix_lasso$table
conf_matrix_table
```

```{r}
# Create dataframe with predictions and actual values to calculate balanced accuracy
data_lasso <- data.frame(
  actual_data = factor(test_Y, levels = c(0, 1)),
  prediction = factor(lasso_predictions, levels = c(0, 1))
)

# Calculate balanced accuracy
balanced_acc <- bal_accuracy(data_lasso, truth = actual_data, estimate = prediction)
print(balanced_acc)
```

#### Random Forest for Feature Selection

```{r}
library(randomForest)
library(caret)
```

For a random forest of classification trees, we usually use a random selection of m= sqrt(p) predictors as split candidates each time a split in a tree is considered. However in our case, this would mean m = sqrt(100,000) = \~316, which is way to computationally expensive. Thus we will apply a pre-selection of features by removing those features with a variance below 0.01 - given the low variance, these features will in no case serve as good predictors for our classification task.

```{r}
# Calculate the variance of each feature
feature_variances <- apply(train_X, 2, var)

# Identify features with variance >= 0.01
selected_features <- which(feature_variances >= 0.01)

# Print the number of features removed
cat("Number of features removed:", ncol(train_X) - length(selected_features), "\n")
cat("Number of features retained:", length(selected_features), "\n")
```

sqrt(30,121) still leaves \~173 features to be considered at each split. This is still too computationally expensive, thus we will try m= 2, 5 and 10 and choose the model with the best performance.

```{r}
# Define the hyperparameter grid for tuning
tune_grid <- expand.grid(
  mtry = c(2, 5, 10)  # Number of features considered as candidates each time the tree is splitt
)
```

We will use the balanced accuracy as our performance metric, because the dataset is highly imbalanced.

```{r}
# Write balanced accuracy as a function that can be accessed by trainControl
calculate_balanced_accuracy <- function(data, lev = NULL, model = NULL) {
  confusion_matrix <- table(data$obs, data$pred)
  TP <- confusion_matrix[2, 2]
  FN <- confusion_matrix[2, 1]
  TN <- confusion_matrix[1, 1]
  FP <- confusion_matrix[1, 2]

  sensitivity <- TP / (TP + FN)  # True Positive Rate
  specificity <- TN / (TN + FP)  # True Negative Rate
  balanced_acc <- (sensitivity + specificity) / 2

  return(c(BalancedAccuracy = balanced_acc))
}
```

We are using a 5-fold Cross-validation to find the optimal split nodes and values.

```{r}
# Set up cross-validation
control <- trainControl(
  method = "cv",            # Cross-validation
  number = 5,               # 5-fold CV
  verboseIter = TRUE,       # Print progress
  savePredictions = "final", # Save predictions
  summaryFunction = calculate_balanced_accuracy
)
```

We are training the model on our training dataset that only contains the selected features (variance \> 0.01). We set the number of trees to be created to 200.

```{r}
# Create new training data set with only selected features
train_X <- train_X[, selected_features]

# Train the model
set.seed(42)
rf_tuned_model <- train(
  x = train_X,
  y = train_Y,
  method = "rf",
  metric = "BalancedAccuracy",      # Metric to optimize
  tuneGrid = tune_grid,     # Hyperparameter grid
  trControl = control,
  ntree = 200,             # Number of trees
  nodesize = 10
)

# Display the best parameters
print(rf_tuned_model$bestTune)
```

```{r}
# Predicting on test set
test_X <- test_X[, selected_features]
rf_predictions <- predict(rf_tuned_model, newdata = test_X)
```

```{r}
# Confusion matrix
conf_matrix_rf <- confusionMatrix(rf_predictions, test_Y)

# Display confusion matrix
print(conf_matrix_rf)
```

```{r}
# Create dataframe with predictions and actual values to calculate balanced accuracy
data_rf <- data.frame(
  actual_data = factor(test_Y, levels = c(0, 1)),
  prediction = factor(rf_predictions, levels = c(0, 1))
)

# Calculate balanced accuracy
balanced_acc <- bal_accuracy(data_rf, truth = actual_data, estimate = prediction)
print(balanced_acc)
```

#### Recursive Feature Elimination with Gradient Boosting Machine

```{r}
library (gbm)

# Create training and testing datasets
train_data <- MLData_Task2[train_indices, ]
test_data <- MLData_Task2[-train_indices, ]

# Define the number of trees
n_trees = 100

# Define grid of lambda (shrinkage) values to evaluate
lambda_grid <- c(0.001,0.01,0.1)

# Initialize a vector to store test errors for each lambda
test_errors <- numeric(length(lambda_grid))
```

```{r}
# Loop over each lambda value
for (i in seq_along(lambda_grid)) {
  lambda <- lambda_grid[i]
  
  # Train the gbm model with the current lambda (shrinkage) value
  gbm_model <- gbm(formula = train_data$label ~ ., 
                   data = train_data[, -1],
                   distribution = "bernoulli", 
                   n.trees = num_trees, 
                   interaction.depth = 5, 
                   shrinkage = lambda, 
                   cv.folds = 5, 
                   verbose = TRUE)
  
  # Make predictions on the test set using the optimal number of trees
  predictions <- predict(gbm_model, newdata = test_X, n.trees = num_trees)
  
  # Calculate the Mean Squared Error on the test set
  test_errors[i] <- mean((predictions - test_Y)^2)
}
```


### Gradient Boosting Feature Selection 

```{r}
# Load required libraries
library(caret)       # For training and evaluation
library(glmnet)      # For Lasso and Ridge regression
library(gbm)         # For Gradient Boosting
library(dplyr)       # For data manipulation
```

```{r}
# Set seed for reproducibility
set.seed(123)

# Split data into training and testing sets (80-20 split)
train_indices <- sample(1:nrow(MLData_Task2), size = 0.8 * nrow(MLData_Task2))
GBtrain_data <- MLData_Task2[train_indices, ]
GBtest_data <- MLData_Task2[-train_indices, ]

# Check the dimensions
cat("Training data dimensions:", dim(GBtrain_data), "\n")
cat("Testing data dimensions:", dim(GBtest_data), "\n")
```

```{r}
# Separate features and labels
GBtrain_X <- as.matrix(GBtrain_data[, -ncol(GBtrain_data)])  # Exclude label column
GBtrain_Y <- factor(GBtrain_data$label)                     # Convert to factor
GBtest_X <- as.matrix(GBtest_data[, -ncol(GBtest_data)])    # Exclude label column
GBtest_Y <- factor(GBtest_data$label)                      # Convert to factor
```

```{r}
# Check the dimensions and class balance
cat("Training features dimensions:", dim(GBtrain_X), "\n")
cat("Testing features dimensions:", dim(GBtest_X), "\n")
cat("Training class distribution:\n")
print(table(GBtrain_Y))
cat("Testing class distribution:\n")
print(table(GBtest_Y))
```


```{r}
# Remove zero-variance columns
zero_var_cols <- nearZeroVar(GBtrain_X, saveMetrics = TRUE)$nzv
GBtrain_X <- GBtrain_X[, !zero_var_cols]
GBtest_X <- GBtest_X[, !zero_var_cols]
```

```{r}
# Verify dimensions after removing zero-variance columns
cat("Training data dimensions after zero-variance removal:", dim(GBtrain_X), "\n")
cat("Test data dimensions after zero-variance removal:", dim(GBtest_X), "\n")
```

```{r}
# Applying PCA to reduce dimensions
set.seed(123)
pca_model <- prcomp(GBtrain_X, scale. = TRUE)

# Retaining components explaining 95% of variance
explained_variance <- cumsum(pca_model$sdev^2) / sum(pca_model$sdev^2)
num_components <- which(explained_variance >= 0.95)[1]
cat("Number of components explaining 95% variance:", num_components, "\n")

# Transforming training and test data using PCA
GBtrain_X_pca <- pca_model$x[, 1:num_components]
GBtest_X_pca <- predict(pca_model, newdata = GBtest_X)[, 1:num_components]

# Verifying PCA-transformed dimensions
cat("Training data PCA-transformed dimensions:", dim(GBtrain_X_pca), "\n")
cat("Test data PCA-transformed dimensions:", dim(GBtest_X_pca), "\n")
```

```{r}
#labels are numeric (0, 1) for binary classification
GBtrain_data_pca <- data.frame(GBtrain_X_pca, label = GBtrain_Y)
GBtest_data_pca <- data.frame(GBtest_X_pca, label = GBtest_Y)

# Converting labels to numeric (0, 1)
GBtrain_data_pca$label <- as.numeric(GBtrain_data_pca$label) - 1  # Convert factor to {0, 1}
GBtest_data_pca$label <- as.numeric(GBtest_data_pca$label) - 1    # Convert factor to {0, 1}

# Check 
cat("Unique values in training labels after conversion:", unique(GBtrain_data_pca$label), "\n")
cat("Unique values in test labels after conversion:", unique(GBtest_data_pca$label), "\n")
```

```{r}
# Ensuring label is a factor with two levels (0 and 1)
GBtrain_data_pca$label <- factor(GBtrain_data_pca$label, levels = c(0, 1))
GBtest_data_pca$label <- factor(GBtest_data_pca$label, levels = c(0, 1))
```

```{r}
# Ensuring that factor levels are valid R variable names
levels(GBtrain_data_pca$label) <- make.names(levels(GBtrain_data_pca$label))
levels(GBtest_data_pca$label) <- make.names(levels(GBtest_data_pca$label))
```

```{r}
# Set up cross-validation with class probabilities
GBcontrol <- trainControl(
  method = "cv",           # Cross-validation
  number = 5,              # 5-fold CV
  search = "grid",         # Grid search for hyperparameters
  verboseIter = TRUE,      # Print progress
  classProbs = TRUE,       # Enable class probability estimation for classification
  summaryFunction = twoClassSummary # Use twoClassSummary to calculate performance metrics (ROC, Sens, Spec)
)
```

```{r}
#Defining a range of shrinkage values to try
shrinkage_values <- c(0.01, 0.05, 0.1, 0.2)
```

```{r}
# Creating a grid of parameters to search
GBtune_grid <- expand.grid(
  shrinkage = shrinkage_values,  # Learning rate
  n.trees = 100,                 # Number of trees
  interaction.depth = 3,         # Tree depth
  n.minobsinnode = 10           # Minimum observations in terminal nodes
)
```

```{r}
# Train the GBM model with cross-validation
set.seed(123)
gbm_tuned_model <- train(
  label ~ .,                         # Formula for Gradient Boosting
  data = GBtrain_data_pca,           # PCA-transformed training data
  method = "gbm",                    # Gradient Boosting Machine
  trControl = GBcontrol,               # Cross-validation settings
  tuneGrid = GBtune_grid,              # Hyperparameter grid
  metric = "ROC",                    # Metric to optimize (ROC is suitable for classification)
  verbose = FALSE                    # Suppress output
)
```


```{r}
# Print the best tuning parameters
print(gbm_tuned_model$bestTune)

# Use the best model to predict on the test set
gbm_predictions <- predict(gbm_tuned_model, newdata = GBtest_data_pca)
```

```{r}
# Confusion matrix and performance metrics
conf_matrix <- confusionMatrix(gbm_predictions, GBtest_data_pca$label)

# Print the confusion matrix and balanced accuracy
print(conf_matrix)
```

```{r}
# Calculate Balanced Accuracy manually from confusion matrix
TP <- conf_matrix$table[2, 2]  # True Positives
TN <- conf_matrix$table[1, 1]  # True Negatives
FP <- conf_matrix$table[1, 2]  # False Positives
FN <- conf_matrix$table[2, 1]  # False Negatives

sensitivity <- TP / (TP + FN)
specificity <- TN / (TN + FP)
GBbalanced_accuracy <- (sensitivity + specificity) / 2

cat("Balanced Accuracy:", GBbalanced_accuracy, "\n")
```

