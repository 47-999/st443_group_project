---
title: "ST443 Group Project - Task 2"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

# ST443 Group Project

# Task 2: Feature selection

## Introduction

The aim of task 2 is to select properties of a molecule in a compound or random probe determine whether a compound binds to a target site on thrombin. This knowledge is required to design new compounds that can be used in drugs.

## T1.1 Data Preparation and Summary Statistics

```{r}
# Load all libraries required to execute the code in this notebook
library(ggplot2)
library(dplyr)
library(caret)
library(glmnet) # To run lasso logistic regression
library(pROC) # To plot ROC curve
library(yardstick) # To calculate balanced accuracy
```

```{r}
# Load and view the data
MLData_Task2 <- read.csv("data2.csv.gz", header=TRUE)
View(MLData_Task2)
```

#### Dataset-level statistics

```{r}
# Check for any missing valus in the dataset
any(is.na(MLData_Task2))
```

```{r}
# Calculate the Feature-Row ratio
cat("Number of features:", ncol(MLData_Task2), "\n")
cat("Number of samples:", nrow(MLData_Task2), "\n")
cat("Feature-to-sample ratio:", ncol(MLData_Task2) / nrow(MLData_Task2), "\n")
```

```{r}
# Check if the dataset is balanced, i.e. if the frequency of each class is approx. the same
table(MLData_Task2$label)
```

We can see that the data is heavily imbalanced - it contains much more observations of class -1 than of class 1.

```{r}
# Calculate the overall sparsity of the dataset (proportion of zeros in the dataset)
sum(MLData_Task2 == 0) / (nrow(MLData_Task2) * ncol(MLData_Task2))
```

As we can see, the dataset is very sparse.

#### Feature-level Statistics

```{r}
# Variance-Based Feature Selection
feature_variances <- apply(MLData_Task2, 2, var)
cat("Proportion of low-variance features (< 0.01):", mean(feature_variances < 0.01), "\n")
```

## T2.2 Training and Evaluation of Feature Selection methods

### Split data into Train and Test data

```{r}
# Random split (80% training, 20% testing)
set.seed(123)
train_indices <- sample(1:nrow(MLData_Task2), size = 0.8 * nrow(MLData_Task2))
```

```{r}
# Create training and testing datasets
train2_data <- MLData_Task2[train_indices, ]
test2_data <- MLData_Task2[-train_indices, ]

# Prepare data
train_X <- as.matrix(train2_data[, -1])  # Exclude label column
test_X <- as.matrix(test2_data[, -1])  # Exclude label column
train_Y <- factor(train2_data$label, levels = c(-1, 1), labels = c(0, 1))  # Convert to 0 and 1
test_Y <- factor(test2_data$label, levels = c(-1, 1), labels = c(0, 1))  # Convert to 0 and 1

# Verify split
cat("Training set size:", nrow(train2_data), "\n")
cat("Test set size:", nrow(test2_data), "\n")
```

### Lasso with Logistic Regression

At first, we use cross-validation on the full feature set to determine the best penalty coefficient lambda for the dataset.

```{r}
# Train Lasso Regression with cross-validation
set.seed(123)
lasso_cv_full <- cv.glmnet(train_X, train_Y, alpha = 1, family = "binomial")
```

```{r}
# Best lambda from cross-validation (the one with the lowest cross-validation error)
# For comment: The larger the lambda, the more coefficients have been sunk to zero
best_lambda <- lasso_cv_full$lambda.min
cat("Best lambda selected:", best_lambda, "\n")
```

As we realised that our dataset is heavily imbalanced, i.e. there are much more observations of class 0 than class 1, we take that into account when fitting our lasso model with the selected best lambda.

```{r}
# Calculate weights
n_class0 <- 580  # Number of samples in class 0
n_class1 <- 60   # Number of samples in class 1
n_total <- n_class0 + n_class1

weight_class0 <- n_total / n_class0
weight_class1 <- n_total / n_class1

# Assign weights to each observation
weights <- ifelse(train_Y == 0, weight_class0, weight_class1)
lasso_model <- glmnet(train_X, train_Y, alpha = 1, family = "binomial", lambda = best_lambda, weights = weights)
```

Predict the probabilities on the test data

```{r}
# Predict probabilities for the positive class
lasso_probs <- predict(lasso_model, newx= test_X, s= best_lambda, type = "response")
# Ensure that the outout is a numeric vector
lasso_probs <- as.numeric(lasso_probs)
```

Now we convert the predicted probabilities into the class labels at the optimal threshold.

```{r}
# Convert probabilities to class labels at optimal threshold
lasso_predictions <- ifelse(lasso_probs > 0.01, 0, 1) 
lasso_predictions <- as.factor(lasso_predictions)
```

Assessment of our model:

```{r}
# Compute confusion matrix
conf_matrix_lasso <- confusionMatrix(lasso_predictions, test_Y)
print(conf_matrix_lasso)

# Extract the confusion matrix table
conf_matrix_table <- conf_matrix_lasso$table
conf_matrix_table
```

```{r}
# Create dataframe with predictions and actual values to calculate balanced accuracy
data_lasso <- data.frame(
  actual_data = factor(test_Y, levels = c(0, 1)),
  prediction = factor(lasso_predictions, levels = c(0, 1))
)

# Calculate balanced accuracy
balanced_acc <- bal_accuracy(data_lasso, truth = actual_data, estimate = prediction)
print(balanced_acc)
```

#### Random Forest for Feature Selection

```{r}
library(randomForest)
library(caret)
```

For a random forest of classification trees, we usually use a random selection of m= sqrt(p) predictors as split candidates each time a split in a tree is considered. However in our case, this would mean m = sqrt(100,000) = \~316, which is way to computationally expensive. Thus we will apply a pre-selection of features by removing those features with a variance below 0.01 - given the low variance, these features will in no case serve as good predictors for our classification task.

```{r}
# Calculate the variance of each feature
feature_variances <- apply(train_X, 2, var)

# Identify features with variance >= 0.01
selected_features <- which(feature_variances >= 0.01)

# Print the number of features removed
cat("Number of features removed:", ncol(train_X) - length(selected_features), "\n")
cat("Number of features retained:", length(selected_features), "\n")
```

sqrt(30,121) still leaves \~173 features to be considered at each split. This is still too computationally expensive, thus we will try m= 2, 5 and 10 and choose the model with the best performance.

```{r}
# Define the hyperparameter grid for tuning
tune_grid <- expand.grid(
  mtry = c(2, 5, 10)  # Number of features considered as candidates each time the tree is splitt
)
```

We will use the balanced accuracy as our performance metric, because the dataset is highly imbalanced.

```{r}
# Write balanced accuracy as a function that can be accessed by trainControl
calculate_balanced_accuracy <- function(data, lev = NULL, model = NULL) {
  confusion_matrix <- table(data$obs, data$pred)
  TP <- confusion_matrix[2, 2]
  FN <- confusion_matrix[2, 1]
  TN <- confusion_matrix[1, 1]
  FP <- confusion_matrix[1, 2]

  sensitivity <- TP / (TP + FN)  # True Positive Rate
  specificity <- TN / (TN + FP)  # True Negative Rate
  balanced_acc <- (sensitivity + specificity) / 2

  return(c(BalancedAccuracy = balanced_acc))
}
```

We are using a 5-fold Cross-validation to find the optimal split nodes and values.

```{r}
# Set up cross-validation
control <- trainControl(
  method = "cv",            # Cross-validation
  number = 5,               # 5-fold CV
  verboseIter = TRUE,       # Print progress
  savePredictions = "final", # Save predictions
  summaryFunction = calculate_balanced_accuracy
)
```

We are training the model on our training dataset that only contains the selected features (variance \> 0.01). We set the number of trees to be created to 200.

```{r}
# Create new training data set with only selected features
train_X <- train_X[, selected_features]

# Train the model
set.seed(42)
rf_tuned_model <- train(
  x = train_X,
  y = train_Y,
  method = "rf",
  metric = "BalancedAccuracy",      # Metric to optimize
  tuneGrid = tune_grid,     # Hyperparameter grid
  trControl = control,
  ntree = 200,             # Number of trees
  nodesize = 10
)

# Display the best parameters
print(rf_tuned_model$bestTune)
```

```{r}
# Predicting on test set
test_X <- test_X[, selected_features]
rf_predictions <- predict(rf_tuned_model, newdata = test_X)
```

```{r}
# Confusion matrix
conf_matrix_rf <- confusionMatrix(rf_predictions, test_Y)

# Display confusion matrix
print(conf_matrix_rf)
```

```{r}
# Create dataframe with predictions and actual values to calculate balanced accuracy
data_rf <- data.frame(
  actual_data = factor(test_Y, levels = c(0, 1)),
  prediction = factor(rf_predictions, levels = c(0, 1))
)

# Calculate balanced accuracy
balanced_acc <- bal_accuracy(data_rf, truth = actual_data, estimate = prediction)
print(balanced_acc)
```

#### Recursive Feature Elimination with Gradient Boosting Machine

```{r}
library (gbm)

# Create training and testing datasets
train_data <- MLData_Task2[train_indices, ]
test_data <- MLData_Task2[-train_indices, ]

# Define the number of trees
n_trees = 100

# Define grid of lambda (shrinkage) values to evaluate
lambda_grid <- c(0.001,0.01,0.1)

# Initialize a vector to store test errors for each lambda
test_errors <- numeric(length(lambda_grid))
```

```{r}
# Loop over each lambda value
for (i in seq_along(lambda_grid)) {
  lambda <- lambda_grid[i]
  
  # Train the gbm model with the current lambda (shrinkage) value
  gbm_model <- gbm(formula = train_data$label ~ ., 
                   data = train_data[, -1],
                   distribution = "bernoulli", 
                   n.trees = num_trees, 
                   interaction.depth = 5, 
                   shrinkage = lambda, 
                   cv.folds = 5, 
                   verbose = TRUE)
  
  # Make predictions on the test set using the optimal number of trees
  predictions <- predict(gbm_model, newdata = test_X, n.trees = num_trees)
  
  # Calculate the Mean Squared Error on the test set
  test_errors[i] <- mean((predictions - test_Y)^2)
}
```


### Gradient Boosting Feature Selection 

```{r}
# Load required libraries
library(caret)       # For training and evaluation
library(glmnet)      # For Lasso and Ridge regression
library(gbm)         # For Gradient Boosting
library(dplyr)       # For data manipulation
```

```{r}
# Set seed for reproducibility
set.seed(123)

# Split data into training and testing sets (80-20 split)
train_indices <- sample(1:nrow(MLData_Task2), size = 0.8 * nrow(MLData_Task2))
GBtrain_data <- MLData_Task2[train_indices, ]
GBtest_data <- MLData_Task2[-train_indices, ]

# Check the dimensions
cat("Training data dimensions:", dim(GBtrain_data), "\n")
cat("Testing data dimensions:", dim(GBtest_data), "\n")
```

```{r}
# Separate features and labels
GBtrain_X <- as.matrix(GBtrain_data[, -ncol(GBtrain_data)])  # Exclude label column
GBtrain_Y <- factor(GBtrain_data$label)                     # Convert to factor
GBtest_X <- as.matrix(GBtest_data[, -ncol(GBtest_data)])    # Exclude label column
GBtest_Y <- factor(GBtest_data$label)                      # Convert to factor
```

```{r}
# Check the dimensions and class balance
cat("Training features dimensions:", dim(GBtrain_X), "\n")
cat("Testing features dimensions:", dim(GBtest_X), "\n")
cat("Training class distribution:\n")
print(table(GBtrain_Y))
cat("Testing class distribution:\n")
print(table(GBtest_Y))
```


```{r}
# Remove zero-variance columns
zero_var_cols <- nearZeroVar(GBtrain_X, saveMetrics = TRUE)$nzv
GBtrain_X <- GBtrain_X[, !zero_var_cols]
GBtest_X <- GBtest_X[, !zero_var_cols]
```

```{r}
# Verify dimensions after removing zero-variance columns
cat("Training data dimensions after zero-variance removal:", dim(GBtrain_X), "\n")
cat("Test data dimensions after zero-variance removal:", dim(GBtest_X), "\n")
```

```{r}
# Applying PCA to reduce dimensions
set.seed(123)
pca_model <- prcomp(GBtrain_X, scale. = TRUE)

# Retaining components explaining 95% of variance
explained_variance <- cumsum(pca_model$sdev^2) / sum(pca_model$sdev^2)
num_components <- which(explained_variance >= 0.95)[1]
cat("Number of components explaining 95% variance:", num_components, "\n")

# Transforming training and test data using PCA
GBtrain_X_pca <- pca_model$x[, 1:num_components]
GBtest_X_pca <- predict(pca_model, newdata = GBtest_X)[, 1:num_components]

# Verifying PCA-transformed dimensions
cat("Training data PCA-transformed dimensions:", dim(GBtrain_X_pca), "\n")
cat("Test data PCA-transformed dimensions:", dim(GBtest_X_pca), "\n")
```

```{r}
#labels are numeric (0, 1) for binary classification
GBtrain_data_pca <- data.frame(GBtrain_X_pca, label = GBtrain_Y)
GBtest_data_pca <- data.frame(GBtest_X_pca, label = GBtest_Y)

# Converting labels to numeric (0, 1)
GBtrain_data_pca$label <- as.numeric(GBtrain_data_pca$label) - 1  # Convert factor to {0, 1}
GBtest_data_pca$label <- as.numeric(GBtest_data_pca$label) - 1    # Convert factor to {0, 1}

# Check 
cat("Unique values in training labels after conversion:", unique(GBtrain_data_pca$label), "\n")
cat("Unique values in test labels after conversion:", unique(GBtest_data_pca$label), "\n")
```

```{r}
# Training GBM model on PCA-reduced data
set.seed(123)
gbm_model <- gbm(
  label ~ .,                         # Formula for Gradient Boosting
  data = GBtrain_data_pca,           # PCA-transformed training data
  distribution = "bernoulli",        # Binary classification
  n.trees = 100,                     # Number of trees
  interaction.depth = 3,             # Tree depth
  shrinkage = 0.05,                  # Learning rate
  n.minobsinnode = 10,               # Minimum observations in terminal nodes
  cv.folds = 5,                      # Cross-validation folds
  verbose = FALSE                    # Suppress output
)
```

```{r}
# Determine the optimal number of trees
best_trees <- gbm.perf(gbm_model, method = "cv")
cat("Optimal number of trees:", best_trees, "\n")
```

```{r}
# Predict probabilities on the test data
gbm_predictions <- predict(
  gbm_model,
  newdata = GBtest_data_pca,
  n.trees = best_trees,
  type = "response"
)
```

```{r}
# Convert probabilities to binary predictions
threshold <- 0.5  # Default threshold
predicted_classes <- ifelse(gbm_predictions > threshold, 1, 0)
predicted_classes <- factor(predicted_classes, levels = c(0, 1))
```

```{r}
# Confusion matrix and performance metrics
conf_matrix <- table(Predicted = predicted_classes, Actual = GBtest_data_pca$label)
TP <- conf_matrix[2, 2]
TN <- conf_matrix[1, 1]
FP <- conf_matrix[1, 2]
FN <- conf_matrix[2, 1]

# Calculate sensitivity, specificity, and balanced accuracy
sensitivity <- TP / (TP + FN)
specificity <- TN / (TN + FP)
balanced_accuracy <- (sensitivity + specificity) / 2

cat("Balanced Accuracy:", balanced_accuracy, "\n")
```

