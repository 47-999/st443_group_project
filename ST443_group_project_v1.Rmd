---
title: "ST443 Group Project"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

# ST443 Group Project

# Task 1: Binary classification

# Introduction

Load and View data:

```{r}
#setwd("C:/Users/nagel freya/OneDrive - The Boston Consulting Group, Inc/Documents/01 Master/01 Uni/01 Term/ST443 Machine Learning and Data Mining/00 Group Project")
RNAdata <-read.csv("data1.csv.gz", header=TRUE)
View(RNAdata)
```

Check for missing data:

```{r}
any(is.na(RNAdata))
```

No data is missing.

## T1.1 Data preparation and Summary Statistics

### Balance / share of different celltypes in dataset

We first create a new column that assigns each celltype (CD4+T and TREG) a numerical value, such that we can calculate statistics that are interesting to us with respect to each different celltype.

```{r}
RNAdata$label <- as.numeric(factor(RNAdata$label))
table(RNAdata$label)
table(RNAdata$label) / nrow(RNAdata)
```

We find that the dataset contains 3356 (\~61.34%) CD4+T cells (numerical value = 1) and 2115 (38.65%) TREG cells (numerical value = 2).

### Significance of genes for different celltypes

Now we generate a new dataset that contains the mean and variance of each gene's expression across cells and for each celltype individually. This will help us later understand the significance of certain gene expression levels for the different celltypes.

```{r}
gene_stats <- data.frame(
  Mean = colMeans(RNAdata[,-1], na.rm = TRUE),
  Variance = apply(RNAdata[,-1], 2, var, na.rm = TRUE),
# Subset for CD4+T cells
  Mean_CD4 = colMeans(RNAdata[RNAdata$label == 1, -1], na.rm = TRUE),
  Variance_CD4 = apply(RNAdata[RNAdata$label == 2, -1], 2, var, na.rm = TRUE),
  # Subset for TREG cells
  Mean_TREG = colMeans(RNAdata[RNAdata$label == 1, -1], na.rm = TRUE),
  Variance_TREG = apply(RNAdata[RNAdata$label == 2, -1], 2, var, na.rm = TRUE)
)
View(gene_stats)
```

### Understand Gene Expression patterns

To understand the Gene Expression patterns we create histograms for a few genes. To understand whether the gene expression patterns are inherently different for the two celltypes, we use the subsets of data for each celltype and compare the histograms.

```{r}
# Create subsets
CD4_data <- subset(RNAdata, label == 1)
TREG_data <- subset(RNAdata, label == 2)

# Pick 4 genes at random
gene_columns <- colnames(RNAdata)[-1]
random_genes <- sample(gene_columns, 4)

par(mfrow = c(4, 3), mar = c(4, 4, 2, 1)) 

for (gene in random_genes) {
  
  # Histogram for gene over all celltypes
  hist(RNAdata[[gene]],
       main = paste("All Cells -", gene),
       xlab = "Expression",
       col = "blue",
       breaks = 20)
  
  # Histogram for gene over CD4+T cells
  hist(CD4_data[[gene]],
       main = paste("CD4+T -", gene),
       xlab = "Expression",
       col = "green",
       breaks = 20)

  # Histogram for gene over TREG cells
  hist(TREG_data[[gene]],
       main = paste("TREG - ", gene),
       xlab = "Expression",
       col = "yellow",
       breaks = 20)
}
```

We can see that there is no significant difference in the distribution of the expressed genes between CD4+T and TREG cells. We can further see that across all celltypes and genes, there is a high number of cells without expression of that specific gene (expression = 0). For the rest of the cells, the expression follows a normal distribution around some mean between 2 and 5.

### Sparsity of Data

RNA data is often sparse, i.e. has a significant number of null-values - our histogram plots above already indicate this. We calculate the sparsity of our dataframe by dividing the number of zero entries by the total number of entries.

```{r}
sum(RNAdata == 0)/(dim(RNAdata)[1]*dim(RNAdata)[2])
```

We get that 66.17% of our entries are nullvalues, meaning that our data is very sparse and thus less variant, as most entries are repeated.

## T1.2 Training and evaluation of classifiers

### Linear Discriminant Analysis (LDA)

```{r}
#Linear Discriminant Analysis 
# Loading necessary libraries
library(MASS)
library(caret)
library(pROC)
```

```{r}
set.seed(123)  # For reproducibility

# Create training (80%) and testing (20%) sets
sample_index <- createDataPartition(RNAdata$label, p = 0.8, list = FALSE)
train_data1 <- RNAdata[sample_index, ]
test_data1 <- RNAdata[-sample_index, ]
```

```{r}
# Train the LDA model
lda_model <- lda(label ~ ., data = train_data1)
print(lda_model)
```

```{r}
# Make predictions on the test data
lda_predictions <- predict(lda_model, test_data1)
predicted_labels <- lda_predictions$class
```

```{r}
# Confusion matrix
conf_matrix <- confusionMatrix(predicted_labels, as.factor(test_data1$label))
conf_matrix
```

```{r}
# Extracting specific metrics
accuracy_lda <- conf_matrix$overall['Accuracy']
balanced_accuracy_lda <- conf_matrix$byClass['Balanced Accuracy']
f1_score_lda <- conf_matrix$byClass['F1']
```

```{r}
cat("Accuracy:", accuracy_lda, "\n")
cat("Balanced Accuracy:", balanced_accuracy_lda, "\n")
cat("F1 Score:", f1_score_lda, "\n")
```

```{r}
# Convert labels to numeric for ROC curve calculation
#test_data1_numeric <- ifelse(test_data1$label == "TREG", 1, 0)
lda_probs <- lda_predictions$posterior[, 2]

# Create ROC curve
roc_curve_lda <- roc(as.factor(test_data1$label), lda_probs)
plot(roc_curve_lda, col = "pink", main = "ROC Curve for LDA")
auc_value_lda <- auc(roc_curve_lda)
cat("AUC:", auc_value_lda, "\n")
```

```{r}
cat("LDA Model Evaluation Summary:\n")
cat("Accuracy:", round(accuracy_lda, 3), "\n")
cat("Balanced Accuracy:", round(balanced_accuracy_lda, 3), "\n")
cat("F1 Score:", round(f1_score_lda, 3), "\n")
cat("AUC:", round(auc_value_lda, 3), "\n")
```

### Random Forest

```{r}
install.packages("randomForest")
```

```{r}
library(randomForest)
```

```{r}
train_data1$label <- as.factor(train_data1$label)
test_data1$label <- as.factor(test_data1$label)
```

```{r}
# Train Random Forest with mtry = sqrt(number of predictors)
set.seed(1)  # For reproducibility
p <- ncol(train_data1) - 1  # Number of predictors (excluding the label column)
rf_model <- randomForest(label ~ ., data = train_data1, mtry = round(sqrt(p)), ntree = 500, importance = TRUE)

# Print the model summary
print(rf_model)
```

```{r}
# Predictions on the test data
rf_predictions <- predict(rf_model, newdata = test_data1)

# Evaluating the model with a confusion matrix
conf_matrix_rf <- confusionMatrix(rf_predictions, as.factor(test_Random_forest_data$label))
print(conf_matrix_rf)
```

```{r}
# Extracting specific performance metrics
accuracy_rf <- conf_matrix_rf$overall['Accuracy']
balanced_accuracy_rf <- conf_matrix_rf$byClass['Balanced Accuracy']
f1_score_rf <- conf_matrix_rf$byClass['F1']

cat("Accuracy:", accuracy_rf, "\n")
cat("Balanced Accuracy:", balanced_accuracy_rf, "\n")
cat("F1 Score:", f1_score_rf, "\n")
```

```{r}
# Getting predicted probabilities for "TREG" class
rf_probs <- predict(rf_model, newdata = test_data1, type = "prob")[, "TREG"]
```

```{r}
# Convert labels to numeric for ROC calculation
test_data1 <- ifelse(test_data1$label == "TREG", 1, 0)
```

```{r}
# Create ROC curve
library(pROC)
roc_curve_rf <- roc(test_data_numeric, rf_probs)
plot(roc_curve_rf, col = "blue", main = "ROC Curve for Random Forest")
```

```{r}
#calculating AUC
auc_value_rf <- auc(roc_curve_rf)
cat("AUC:", auc_value_rf, "\n")
```

```{r}
cat("Random Forest Evaluation Summary:\n")
cat("Accuracy:", round(accuracy_rf, 3), "\n")
cat("Balanced Accuracy:", round(balanced_accuracy_rf, 3), "\n")
cat("F1 Score:", round(f1_score_rf, 3), "\n")
cat("AUC:", round(auc_value_rf, 3), "\n")
```

### K-NN

```{r}
library(class)
```

```{r}
# Create matrices for predictors (X)
train_X <- as.matrix(train_data1[, -1])  # Exclude the label column
test_X <- as.matrix(test_data1[, -1])

# Create vectors for labels (Y)
train_Y <- train_data1$label
test_Y <- test_data1$label
```

```{r}
# Setting k = 5
k <- 5

# Perform k-NN classification
knn_predictions <- knn(train_X, test_X, train_Y, k = k)

```

```{r}
# Confusion matrix
conf_matrix_knn <- confusionMatrix(knn_predictions, as.factor(test_Y))
print(conf_matrix_knn)

# Extract the confusion matrix as a table
confusion_table <- conf_matrix_knn$table
print(confusion_table)

#Calculating Accuracy
accuracy_knn <- sum(diag(confusion_table)) / sum(confusion_table)
cat("Accuracy:", accuracy_knn, "\n")

```

```{r}
precision <- conf_matrix_knn$byClass["Precision"]
recall <- conf_matrix_knn$byClass["Recall"]
f1_score_knn <- 2 * ((precision * recall) / (precision + recall))
cat("F1 Score:", f1_score_knn, "\n")
```

```{r}
sensitivity <- conf_matrix_knn$byClass["Sensitivity"]
specificity <- conf_matrix_knn$byClass["Specificity"]
balanced_accuracy_knn <- (sensitivity + specificity) / 2
cat("Balanced Accuracy:", balanced_accuracy_knn, "\n")
```

```{r}
# Calculate pseudo-probabilities for AUC
k <- 5  #The same k as used above
knn_probabilities <- attr(knn_predictions, "prob")
knn_probabilities <- ifelse(knn_predictions == "TREG", knn_probabilities, 1 - knn_probabilities)
summary(knn_probabilities)  # Ensure values range between 0 and 1
```

```{r}
# Check levels of the test_Y factor
levels(test_Y)
table(test_Y)
```

```{r}
test_Y_numeric <- ifelse(test_Y == "TREG", 1, 0)  # Assuming "TREG" is the positive class
table(test_Y_numeric)
```

```{r}
# Calculate AUC
roc_curve_knn <- roc(test_Y_numeric, knn_probabilities)
auc_value_knn <- auc(roc_curve_knn)
cat("AUC:", auc_value_knn, "\n")
```

```{r}
# Plot ROC curve
plot(roc_curve_knn, col = "blue", main = "ROC Curve for k-NN")
```

```{r}
cat("Knn Classifiers Summary:\n")
cat("Accuracy:", round(accuracy_knn, 3), "\n")
cat("Balanced Accuracy:", round(balanced_accuracy_knn, 3), "\n")
cat("F1 Score:", round(f1_score_knn, 3), "\n")
cat("AUC:", round(auc_value_knn, 3), "\n")
```
