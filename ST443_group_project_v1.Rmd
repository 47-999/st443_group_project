---
title: "ST443 Group Project - Task 1"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

# ST443 Group Project

# Task 1: Binary classification

## Introduction

The aim of Task 1 is to classify immune cell types based on RNA expression levels.

Load and View data:

```{r}
RNAdata <-read.csv("data1.csv.gz", header=TRUE)
View(RNAdata)
```

Check for missing data:

```{r}
any(is.na(RNAdata))
```

No data is missing.

Check for infinite values

```{r}
sum(sapply(RNAdata, function(x) sum(is.infinite(x))))
```

No infinite values.

## T1.1 Data preparation and Summary Statistics

### Balance / share of different celltypes in dataset

We first create a new column that assigns each celltype (CD4+T and TREG) a numerical value, such that we can calculate statistics that are interesting to us with respect to each different celltype.

```{r}
RNAdata$label <- as.numeric(factor(RNAdata$label))
table(RNAdata$label)
table(RNAdata$label) / nrow(RNAdata)
```

We find that the dataset contains 3356 (\~61.34%) CD4+T cells (numerical value = 1) and 2115 (38.65%) TREG cells (numerical value = 2).

To visualize this distribution, we generated a bar plot.

```{r}
# Load ggplot2 library for plotting
library(ggplot2)

# Create a table of class counts
class_counts <- table(RNAdata$label)

# Convert the table to a data frame for ggplot
class_counts_df <- as.data.frame(class_counts)
colnames(class_counts_df) <- c("Label", "Count")

# Bar plot for class distribution
ggplot(class_counts_df, aes(x = Label, y = Count, fill = Label)) +
  geom_bar(stat = "identity") +
  labs(title = "Class Distribution", x = "Cell Type", y = "Count") +
  theme_minimal()

```

As seen in the plot, there is a class imbalance, with CD4+T cells being more prevalent than TREG cells. This imbalance could potentially impact the performance of machine learning models, as classifiers might be biased toward the majority class.

### Significance of genes for different celltypes

Now we generate a new dataset that contains the mean and variance of each gene's expression across cells and for each celltype individually. This will help us later understand the significance of certain gene expression levels for the different celltypes.

```{r}
gene_stats <- data.frame(
  Mean = colMeans(RNAdata[,-1], na.rm = TRUE),
  Variance = apply(RNAdata[,-1], 2, var, na.rm = TRUE),
# Subset for CD4+T cells
  Mean_CD4 = colMeans(RNAdata[RNAdata$label == 1, -1], na.rm = TRUE),
  Variance_CD4 = apply(RNAdata[RNAdata$label == 2, -1], 2, var, na.rm = TRUE),
  # Subset for TREG cells
  Mean_TREG = colMeans(RNAdata[RNAdata$label == 1, -1], na.rm = TRUE),
  Variance_TREG = apply(RNAdata[RNAdata$label == 2, -1], 2, var, na.rm = TRUE)
)
View(gene_stats)
```

We also generate a dataset that contains the minimum and the maximum value of each gene.

```{r}
# Calculate the minimum for each gene
min_values <- apply(RNAdata[,-1], 2, min)

# Calculate the maximum for each gene
max_values <- apply(RNAdata[,-1], 2, max)

# Combine the results into a data frame
min_max_values <- data.frame(
  Gene = colnames(RNAdata[,-1]),
  Minimum = min_values,
  Maximum = max_values
)


View(min_max_values)
```

### Understand Gene Expression patterns

To understand the Gene Expression patterns we create histograms for a few genes. To understand whether the gene expression patterns are inherently different for the two celltypes, we use the subsets of data for each celltype and compare the histograms.

```{r}
# Create subsets
CD4_data <- subset(RNAdata, label == 1)
TREG_data <- subset(RNAdata, label == 2)

# Pick 4 genes at random
gene_columns <- colnames(RNAdata)[-1]
random_genes <- sample(gene_columns, 4)

par(mfrow = c(4, 3), mar = c(4, 4, 2, 1)) 

for (gene in random_genes) {
  
  # Histogram for gene over all celltypes
  hist(RNAdata[[gene]],
       main = paste("All Cells -", gene),
       xlab = "Expression",
       col = "blue",
       breaks = 20)
  
  # Histogram for gene over CD4+T cells
  hist(CD4_data[[gene]],
       main = paste("CD4+T -", gene),
       xlab = "Expression",
       col = "green",
       breaks = 20)

  # Histogram for gene over TREG cells
  hist(TREG_data[[gene]],
       main = paste("TREG - ", gene),
       xlab = "Expression",
       col = "yellow",
       breaks = 20)
}
```

We can see that there is no significant difference in the distribution of the expressed genes between CD4+T and TREG cells. We can further see that across all celltypes and genes, there is a high number of cells without expression of that specific gene (expression = 0). For the rest of the cells, the expression follows a normal distribution around some mean between 2 and 5.

# Density plots for genes

The code below generates density plots for the expression levels of randomly selected genes. This helps visualize the distribution of RNA expression for each gene across the two cell types (CD4+T and TREG).

```{r}
# Randomly select 5 genes for visualization
set.seed(Sys.time())  # Set a random seed for reproducibility
selected_genes <- sample(colnames(RNAdata[,-1]), 5)

# Loop through the selected genes and create density plots
for (gene in selected_genes) {
  p <- ggplot(RNAdata, aes(x = .data[[gene]], fill = label)) +  # Use full RNAdata with `label`
    geom_density(alpha = 0.5) +
    labs(title = paste("Density Plot for Gene:", gene), x = "Expression Level", y = "Density") +
    theme_minimal()
  
  print(p)  # Explicitly print each plot
}
```

The density plots for most genes reveal highly overlapping distributions for CD4+T and TREG cells, suggesting that many genes do not exhibit significant differential expression between the two cell types. This overlap implies that these genes may not be strong discriminators for classification and might be less relevant for building effective predictive models. Such observations underscore the importance of identifying key genes or features that show greater separation between the classes, as they are likely to contribute more meaningfully to the task of binary classification.

# Scatter plots

The scatter plots also helps us visualize the expression levels of randomly selected pairs of genes, comparing their distributions between the two cell types (CD4+T and TREG)

```{r}
# Randomly select 2 pairs of genes for scatter plots
set.seed(Sys.time())
selected_genes <- sample(colnames(RNAdata[,-1]), 4)

# Create scatter plots for the selected pairs
library(ggplot2)
ggplot(RNAdata, aes(x = .data[[selected_genes[1]]], y = .data[[selected_genes[2]]], color = label)) +
  geom_point(alpha = 0.7) +
  labs(title = paste("Scatter Plot:", selected_genes[1], "vs", selected_genes[2]), 
       x = selected_genes[1], y = selected_genes[2]) +
  theme_minimal()

ggplot(RNAdata, aes(x = .data[[selected_genes[3]]], y = .data[[selected_genes[4]]], color = label)) +
  geom_point(alpha = 0.7) +
  labs(title = paste("Scatter Plot:", selected_genes[3], "vs", selected_genes[4]), 
       x = selected_genes[3], y = selected_genes[4]) +
  theme_minimal()

```

The significant overlap in distributions between CD4+T and TREG cells indicates that these gene pairs may not be highly informative for distinguishing the two classes. This highlights the importance of identifying gene pairs or combinations with stronger class separation for effective feature selection and classification.

### Sparsity of Data

RNA data is often sparse, i.e. has a significant number of null-values - our histogram plots above already indicate this. We calculate the sparsity of our dataframe by dividing the number of zero entries by the total number of entries.

```{r}
sum(RNAdata == 0)/(dim(RNAdata)[1]*dim(RNAdata)[2])
```

We get that 66.17% of our entries are nullvalues, meaning that our data is very sparse and thus less variant, as most entries are repeated.

## T1.2 Training and evaluation of classifiers

### Load Necessary Libraries
```{r}
# Loading necessary libraries
library(MASS)
library(caret)
library(pROC)
library(klaR)
library(randomForest)
library(class)
library(e1071)
library(gbm)
```


### Split data into Train and Test data

```{r}
# Set seed for reproducibility
set.seed(123)  

# Create training (80%) and testing (20%) sets
sample_index <- createDataPartition(RNAdata$label, p = 0.8, list = FALSE)
train_data1 <- RNAdata[sample_index, ]
test_data1 <- RNAdata[-sample_index, ]
```

### Linear Discriminant Analysis (LDA)

```{r}
# Train the LDA model
lda_model <- lda(label ~ ., data = train_data1)
```

```{r}
# Make predictions on the test data
lda_predictions <- predict(lda_model, test_data1[,-1])
lda_predicted_labels <- lda_predictions$class
```

```{r}
# Confusion matrix
lda_conf_matrix <- confusionMatrix(lda_predicted_labels, as.factor(test_data1$label))
lda_conf_matrix
```

```{r}
lda_accuracy <- lda_conf_matrix$overall['Accuracy']
lda_balanced_accuracy <- lda_conf_matrix$byClass['Balanced Accuracy']
lda_f1_score <- lda_conf_matrix$byClass['F1']
```

```{r}
# Convert labels to numeric for ROC curve calculation
lda_probs <- lda_predictions$posterior[, 2]

# Create ROC curve
lda_roc_curve <- roc(as.factor(test_data1$label), lda_probs)
plot(lda_roc_curve, col = "pink", main = "ROC Curve for LDA")
lda_auc_value <- auc(lda_roc_curve)
cat("AUC:", lda_auc_value, "\n")
```

```{r}
cat("LDA Model Evaluation Summary:\n")
cat("Accuracy:", round(lda_accuracy, 3), "\n")
cat("Balanced Accuracy:", round(lda_balanced_accuracy, 3), "\n")
cat("F1 Score:", round(lda_f1_score, 3), "\n")
cat("AUC:", round(lda_auc_value, 3), "\n")
```

### Logistic Classifier

```{r}
# Create training (80%) and testing (20%) set for logistic classifer
train_data_log <- train_data1
test_data_log <- test_data1

train_data_log$label <- ifelse(train_data_log$label == "TREG", 1, 0)
test_data_log$label <- ifelse(test_data_log$label == "TREG", 1, 0)

# Train the logistic regression model
logistic_model <- glm(label ~ ., data = train_data_log, family = binomial)
```

```{r}
# Predict probabilities on the test set
logistic_predictions <- predict(logistic_model, test_data_log[, -1], type = "response")

# Convert probabilities to class labels (threshold = 0.5)
logistic_predicted_labels <- ifelse(logistic_predictions > 0.5, 1, 0)
```

```{r}
# Calculate test error
logistic_test_error <- mean(logistic_predicted_labels != test_data_log$label)

# Print the test error
cat("Test Error (Logistic):", round(logistic_test_error, 3), "\n")
```

```{r}
# Confusion matrix
logistic_conf_matrix <- confusionMatrix(as.factor(logistic_predicted_labels), as.factor(test_data_log$label))
print(logistic_conf_matrix)
```

```{r}
accuracy_logistic <- logistic_conf_matrix$overall['Accuracy']
balanced_accuracy_logistic <- logistic_conf_matrix$byClass['Balanced Accuracy']
f1_score_logistic <- logistic_conf_matrix$byClass['F1']
```

```{r}
# Calculate and plot the ROC curve
roc_curve_logistic <- roc(test_data_log$label, logistic_predictions)
plot(roc_curve_logistic, main = "ROC Curve for Logistic Regression", col='red')
auc_logistic <- auc(roc_curve_logistic)
cat("AUC:", auc_logistic, "\n")
```

```{r}
cat("Logistic Regression Model Evaluation Summary:\n")
cat("Accuracy:", round(accuracy_logistic, 3), "\n")
cat("Balanced Accuracy:", round(balanced_accuracy_logistic, 3), "\n")
cat("F1 Score:", round(f1_score_logistic, 3), "\n")
cat("AUC:", round(auc_logistic, 3), "\n")
```

### Quadratic Discriminant Analysis (QDA)

```{r}
# Create training (80%) and testing (20%) set for qda
test_data_factor <- test_data1
train_data_factor <- train_data1

test_data_factor$label <- factor(test_data_factor$label)
train_data_factor$label <- factor(train_data_factor$label)

# Train a QDA model (using rda from klaR library to account for the high-dimensional data by introducing a small regularization factor to the covariance matrices, making them invertible even with the few samples available)
qda_model <- rda(label ~ ., data = train_data_factor, gamma = 0.01, lambda = 0.5)
```

```{r}
# Predict the label of the test data
qda_predictions <- predict(qda_model, test_data_factor[,-1])  # Exclude label column
qda_predicted_labels <- qda_predictions$class
```

```{r}
# Confusion matrix
qda_conf_matrix <- confusionMatrix(qda_predicted_labels, test_data_factor$label)
qda_conf_matrix
```

```{r}
qda_accuracy <- qda_conf_matrix$overall['Accuracy']
qda_balanced_accuracy <- qda_conf_matrix$byClass['Balanced Accuracy']
qda_f1_score <- qda_conf_matrix$byClass['F1']
```

```{r}
# Calculate and plot the ROC curve
roc_curve_qda <- roc(test_data1$label, qda_predictions)  # Use probabilities for ROC
plot(roc_curve_qda, main = "ROC Curve for QDA", col='red')
auc_qda <- auc(roc_curve_qda)
cat("AUC:", auc_qda, "\n")
```

```{r}
cat("QDA Model Evaluation Summary:\n")
cat("Accuracy:", round(qda_accuracy, 3), "\n")
cat("Balanced Accuracy:", round(qda_balanced_accuracy, 3), "\n")
cat("F1 Score:", round(qda_f1_score, 3), "\n")
cat("AUC:", round(qda_auc_value, 3), "\n")
```

### Random Forest

```{r}
# Train Random Forest with mtry = sqrt(number of predictors)
p <- ncol(train_data1) - 1  # Number of predictors (excluding the label column)
train_data1$label <- as.factor(train_data1$label)
rf_model <- randomForest(label ~ ., data = train_data1, mtry = round(sqrt(p)), ntree = 500, importance = TRUE)

# Print the model summary
print(rf_model)
```

```{r}
# Predictions on the test data
rf_predictions <- predict(rf_model, test_data1)

# Confusion matrix
conf_matrix_rf <- confusionMatrix(rf_predictions, as.factor(test_data1$label))
print(conf_matrix_rf)
```

```{r}
accuracy_rf <- conf_matrix_rf$overall['Accuracy']
balanced_accuracy_rf <- conf_matrix_rf$byClass['Balanced Accuracy']
f1_score_rf <- conf_matrix_rf$byClass['F1']
```

```{r}
# Getting predicted probabilities for "TREG" class
rf_probs <- predict(rf_model, test_data1, type = "prob")[, 2]
```

```{r}
# Create ROC curve
roc_curve_rf <- roc(test_data1$label, rf_probs)
plot(roc_curve_rf, col = "blue", main = "ROC Curve for Random Forest")
```

```{r}
#calculating AUC
auc_value_rf <- auc(roc_curve_rf)
```

```{r}
cat("Random Forest Evaluation Summary:\n")
cat("Accuracy:", round(accuracy_rf, 3), "\n")
cat("Balanced Accuracy:", round(balanced_accuracy_rf, 3), "\n")
cat("F1 Score:", round(f1_score_rf, 3), "\n")
cat("AUC:", round(auc_value_rf, 3), "\n")
```

### K-NN

```{r}
train_X <- as.matrix(train_data1[, -1]) # Excluding the label column
test_X <- as.matrix(test_data1[, -1])

# Create vectors for labels (Y)
train_Y <- as.factor(train_data1$label)
test_Y <- as.factor(test_data1$label)
```

```{r}
cv_control <- trainControl(method = "cv", number = 10)

# Tuning the knn to try k values from 1 to 20
knn_tune <- train(train_X, train_Y, method = "knn", trControl = cv_control,
                  tuneGrid = expand.grid(k = 1:20))
optimal_k <- knn_tune$bestTune$k
print(optimal_k)

# optimal k is 19
```

```{r}
# Predict probabilities on the test set
knn_predictions <- predict(knn_tune, test_X)
knn_probabilities <- predict(knn_tune, test_X, type = "prob")
```

```{r}
# Confusion matrix
conf_matrix_knn <- confusionMatrix(knn_predictions, as.factor(test_Y))
print(conf_matrix_knn)

#Calculating Accuracy, Balanced Accuracy, F1
accuracy_knn <- sum(diag(confusion_table)) / sum(confusion_table)
balanced_accuracy_knn <- conf_matrix_knn$byClass["Balanced Accuracy"]
f1_score_knn <- conf_matrix_knn$byClass["F1"]
```

```{r}
# AUC Calculation
knn_roc_obj <- roc(test_Y, knn_probabilities[, 2])
auc_knn <- auc(knn_roc_obj)

# Plot ROC curve
plot(knn_roc_obj, col = "blue", main = "ROC Curve for KNN")
```

```{r}
cat("KNN Classifiers Summary:\n")
cat("Accuracy:", round(accuracy_knn, 3), "\n")
cat("Balanced Accuracy:", round(balanced_accuracy_knn, 3), "\n")
cat("F1 Score:", round(f1_score_knn, 3), "\n")
cat("AUC:", round(auc_knn, 3), "\n")
```

### Support Vector Machine (SVM)

```{r}
# Train and test data with label as factors
test_data_factor <- test_data1
train_data_factor <- train_data1

test_data_factor$label <- factor(test_data_factor$label)
train_data_factor$label <- factor(train_data_factor$label)

# Model
svm_model <- svm(label ~ ., data = train_data_factor, kernel = "linear", probability = TRUE)
svm_predictions <- predict(svm_model, test_data_factor[,-1], probability = TRUE)
svm_pred_prob <- attr(svm_predictions, "probabilities")[, 2]
```

```{r}
# Accuracy
svm_accuracy <- mean(svm_predictions == test_data_factor$label)

# Confusion matrix
svm_conf_matrix <- confusionMatrix(svm_predictions, test_data_factor$label)

# Balanced accuracy
svm_balanced_accuracy <- svm_conf_matrix$byClass['Balanced Accuracy']

# F1 Score
svm_f1 <- svm_conf_matrix$byClass['F1']

# ROC Curve & AUC
svm_roc_curve <- roc(test_data_factor$label, svm_pred_prob)
plot(svm_roc_curve, main = "ROC Curve", col = "blue", lwd = 2)
svm_auc <- auc(svm_roc_curve)
```
```{r}
cat("Support Vector Machine Model Evaluation Summary:\n")
cat("Accuracy:", round(svm_accuracy, 3), "\n")
cat("Balanced Accuracy:", round(svm_balanced_accuracy, 3), "\n")
cat("F1 Score:", round(svm_f1, 3), "\n")
cat("AUC:", round(svm_auc, 3), "\n")
```

### Gradient Boosting Decision Tree (GBDT)

```{r}
### Tuning - takes a long time to run ***

# Create the tuning grid
tune_grid <- expand.grid(
  n.trees = c(50, 100, 150),
  interaction.depth = c(1, 3, 5),
  shrinkage = c(0.01, 0.1, 0.2),
  n.minobsinnode = c(10, 20) 
)

model <- gbm(
  formula = label ~ .,              
  data = train_data_gbdt,              
  distribution = "bernoulli",    
  n.cores = 1,                   
  verbose = FALSE        
)
train_control <- trainControl(method = "cv", number = 5)
tuned_model <- train(
  label ~ ., 
  data = train_data_gbdt, 
  method = "gbm", 
  trControl = train_control, 
  tuneGrid = tune_grid,
  verbose = FALSE
)
tuned_model
```

Using the above tuning model, we found the optimal n.trees = 150, interactions.depth = 5, shrinkage = 0.1, and 
n.minobsinnode = 20. I used those values for the model below. 

```{r}
test_data_gbdt <- test_data1
train_data_gbdt <- train_data1

test_data_gbdt$label <- as.numeric(as.factor(test_data_gbdt$label)) - 1
train_data_gbdt$label <- as.numeric(as.factor(train_data_gbdt$label)) - 1

#Model
gbm_model <- gbm(label ~ ., data = train_data_gbdt, 
                 distribution = "bernoulli", 
                 n.trees = 150, 
                 interaction.depth = 5, 
                 cv.folds = 5,
                 shrinkage = 0.1,
                 n.minobsinnode = 20,
                 verbose = FALSE)

best_trees <- gbm.perf(gbm_model, method = "cv")

gbdt_predictions <- predict(gbm_model, test_data_gbdt[,-1], n.trees = best_trees, type = "response")

gbdt_predictions_class <- ifelse(gbdt_predictions > 0.5, 1, 0)
```

```{r}
# Accuracy
gbdt_accuracy <- mean(gbdt_predictions_class == test_data_gbdt$label)

# Confusion Matrix
gbdt_conf_matrix <- confusionMatrix(as.factor(gbdt_predictions_class), as.factor(test_data_gbdt$label))

# Balanced Accuracy
gbdt_balanced_accuracy <- mean(gbdt_conf_matrix$byClass["Sensitivity"], gbdt_conf_matrix$byClass["Specificity"])

# F1 Score
gbdt_f1_score <- 2 * (gbdt_conf_matrix$byClass["Precision"] * gbdt_conf_matrix$byClass["Recall"]) / 
  (gbdt_conf_matrix$byClass["Precision"] + gbdt_conf_matrix$byClass["Recall"])

# AUC 
gbdt_roc_curve <- roc(test_data_gbdt$label, gbdt_predictions)
gbdt_auc_value <- auc(gbdt_roc_curve)

# ROC Curve
plot.roc(gbdt_roc_curve, main = "GBDT ROC Curve", col = "blue", lwd = 2)

```
```{r}
cat("Gradient Boosting Decision Trees Model Evaluation Summary:\n")
cat("Accuracy:", round(gbdt_accuracy, 3), "\n")
cat("Balanced Accuracy:", round(gbdt_balanced_accuracy, 3), "\n")
cat("F1 Score:", round(gbdt_f1_score, 3), "\n")
cat("AUC:", round(gbdt_auc_value, 3), "\n")
```


### PCA with 10 components

Split the data:

```{r}
# Perform PCA on the training data (excluding the label column)
pca <- prcomp(train_data1[, -1], scale. = TRUE)

# Keep only the first 10 principal components
train_pca <- data.frame(label = train_data1$label, pca$x[, 1:10])
test_pca <- data.frame(label = test_data1$label, predict(pca, test_data1[, -1])[, 1:10])
```

### LDA with PCA

```{r}
# Train the LDA model on the PCA-transformed training data
lda_pca_model <- lda(label ~ ., data = train_pca)

# Print the LDA model summary
summary(lda_pca_model)
```

```{r}
# Make predictions on the PCA-transformed test data
lda_pca_predictions <- predict(lda_pca_model, test_pca)

# Confusion matrix

conf_matrix_pca <- confusionMatrix(as.factor(lda_pca_predictions$class), as.factor(test_pca$label))
print(conf_matrix_pca)

# Calculate test error
lda_pca_test_error <- mean(lda_pca_predictions$class != test_pca$label)
cat("LDA PCA Test Error:", round(lda_pca_test_error, 3), "\n")

```

```{r}
# Extract metrics from the confusion matrix
accuracy_lda_pca <- conf_matrix_pca$overall['Accuracy']
balanced_accuracy_lda_pca <- conf_matrix_pca$byClass['Balanced Accuracy']
f1_score_lda_pca <- conf_matrix_pca$byClass['F1']

# Print the metrics
cat("Accuracy (LDA with PCA):", accuracy_lda_pca, "\n")
cat("Balanced Accuracy (LDA with PCA):", balanced_accuracy_lda_pca, "\n")
cat("F1 Score (LDA with PCA):", f1_score_lda_pca, "\n")
```

```{r}
# Calculate and plot the ROC curve
roc_curve_pca <- roc(test_pca$label, as.numeric(lda_pca_predictions$class), levels = c("TREG", "CD4+T"))
plot(roc_curve_pca, main = "ROC Curve for LDA with PCA", col='red')
auc_pca <- auc(roc_curve_pca)
cat("AUC (LDA with PCA):", round(auc_pca, 3), "\n")
```

```{r}
cat("LDA with PCA Model Evaluation Summary:\n")
cat("Accuracy:", round(accuracy_lda_pca, 3), "\n")
cat("Balanced Accuracy:", round(balanced_accuracy_lda_pca, 3), "\n")
cat("F1 Score:", round(f1_score_lda_pca, 3), "\n")
cat("AUC:", round(auc_pca, 3), "\n")
```

### Logistic classifier with PCA

```{r}
# Create binary-labeled datasets for logistic regression
train_pca_log <- train_pca
test_pca_log <- test_pca

# Convert 'label' to binary numeric variable
train_pca_log$label <- ifelse(train_pca_log$label == "TREG", 1, 0)
test_pca_log$label <- ifelse(test_pca_log$label == "TREG", 1, 0)

```

```{r}
# Train logistic regression on PCA-transformed data
logistic_pca_model <- glm(label ~ ., data = train_pca_log, family = binomial)

# Print the model summary
summary(logistic_pca_model)

```

```{r}
# Predict probabilities on the PCA-transformed test data
logistic_pca_predictions <- predict(logistic_pca_model, newdata = test_pca_log, type = "response")

# Convert probabilities to class labels (threshold = 0.5)
logistic_pca_predicted_labels <- ifelse(logistic_pca_predictions > 0.5, 1, 0)

# Calculate test error
logistic_pca_test_error <- mean(logistic_pca_predicted_labels != test_pca_log$label)
cat("Test Error (Logistic with PCA):", round(logistic_pca_test_error, 3), "\n")

```

```{r}
# Confusion matrix

conf_matrix_pca <- confusionMatrix(as.factor(logistic_pca_predicted_labels), as.factor(test_pca_log$label))
print(conf_matrix_pca)

# Extract metrics from the confusion matrix
accuracy_logistic_pca <- conf_matrix_pca$overall['Accuracy']
balanced_accuracy_logistic_pca <- conf_matrix_pca$byClass['Balanced Accuracy']
f1_score_logistic_pca <- conf_matrix_pca$byClass['F1']

# Print the metrics
cat("Accuracy (Logistic with PCA):", accuracy_logistic_pca, "\n")
cat("Balanced Accuracy (Logistic with PCA):", balanced_accuracy_logistic_pca, "\n")
cat("F1 Score (Logistic with PCA):", f1_score_logistic_pca, "\n")


```

```{r}
# Calculate and plot the ROC curve

roc_curve_logistic_pca <- roc(test_pca_log$label, logistic_pca_predictions)
plot(roc_curve_logistic_pca, main = "ROC Curve for Logistic Regression with PCA", col = 'red')
auc_logistic_pca <- auc(roc_curve_logistic_pca)
cat("AUC (Logistic with PCA):", auc_logistic_pca, "\n")

```

```{r}
cat("Logistic Regression with PCA Model Evaluation Summary:\n")
cat("Accuracy:", round(accuracy_logistic_pca, 3), "\n")
cat("Balanced Accuracy:", round(balanced_accuracy_logistic_pca, 3), "\n")
cat("F1 Score:", round(f1_score_logistic_pca, 3), "\n")
cat("AUC:", round(auc_logistic_pca, 3), "\n")

```

### QDA with PCA

```{r}
# Train a QDA model on the PCA-transformed data
library(questionr)
library(klaR)

qda_pca_model <- rda(label ~ ., data = train_pca, gamma = 0.01, lambda = 0.5)

# Print the model summary
summary(qda_pca_model)

```

```{r}
# Predict the label of the test data
qda_pca_predictions <- predict(qda_pca_model, test_pca[,-1])  # Exclude label column
qda_pca_predicted_labels <- qda_pca_predictions$class

# Confusion matrix
qda_pca_conf_matrix <- confusionMatrix(as.factor(qda_pca_predicted_labels), as.factor(test_pca$label))
qda_pca_conf_matrix

```

```{r}
# Extract metrics from the confusion matrix
qda_pca_accuracy <- qda_pca_conf_matrix$overall['Accuracy']
qda_pca_balanced_accuracy <- qda_pca_conf_matrix$byClass['Balanced Accuracy']
qda_pca_f1_score <- qda_pca_conf_matrix$byClass['F1']

# Print the metrics
cat("Accuracy (QDA with PCA):", qda_pca_accuracy, "\n")
cat("Balanced Accuracy (QDA with PCA):", qda_pca_balanced_accuracy, "\n")
cat("F1 Score (QDA with PCA):", qda_pca_f1_score, "\n")

```

```{r}
# Calculate the ROC curve
qda_pca_roc_curve <- roc(test_pca$label, qda_pca_predictions$posterior[, "TREG"], levels = c("TREG", "CD4+T"))
plot(qda_pca_roc_curve, main = "ROC Curve for QDA with PCA", col = 'red')
qda_pca_auc <- auc(qda_pca_roc_curve)
cat("AUC (QDA with PCA):", round(qda_pca_auc, 3), "\n")


```

```{r}
cat("QDA with PCA Model Evaluation Summary:\n")
cat("Accuracy:", round(qda_pca_accuracy, 3), "\n")
cat("Balanced Accuracy:", round(qda_pca_balanced_accuracy, 3), "\n")
cat("F1 Score:", round(qda_pca_f1_score, 3), "\n")
cat("AUC:", round(qda_pca_auc, 3), "\n")

```

### Random Forest with PCA

```{r}
library(randomForest)

# Ensure the label column is a factor
train_pca$label <- as.factor(train_pca$label)
test_pca$label <- as.factor(test_pca$label)

# Train Random Forest on PCA-transformed data
p <- ncol(train_pca) - 1  # Number of predictors (10 components)
rf_pca_model <- randomForest(label ~ ., data = train_pca, mtry = round(sqrt(p)), ntree = 500, importance = TRUE)

# Print the model summary
print(rf_pca_model)

```

```{r}
# Predictions on the test data
rf_pca_predictions <- predict(rf_pca_model, test_pca)

# Confusion matrix
conf_matrix_rf_pca <- confusionMatrix(rf_pca_predictions, test_pca$label)
print(conf_matrix_rf_pca)

```

```{r}
# Extract specific performance metrics
accuracy_rf_pca <- conf_matrix_rf_pca$overall['Accuracy']
balanced_accuracy_rf_pca <- conf_matrix_rf_pca$byClass['Balanced Accuracy']
f1_score_rf_pca <- conf_matrix_rf_pca$byClass['F1']

# Print the metrics
cat("Accuracy (Random Forest with PCA):", round(accuracy_rf_pca, 3), "\n")
cat("Balanced Accuracy (Random Forest with PCA):", round(balanced_accuracy_rf_pca, 3), "\n")
cat("F1 Score (Random Forest with PCA):", round(f1_score_rf_pca, 3), "\n")

```

```{r}
# Get predicted probabilities for "TREG" class
rf_pca_probs <- predict(rf_pca_model, test_pca, type = "prob")[, "TREG"]

# Create ROC curve
roc_curve_rf_pca <- roc(test_pca$label, rf_pca_probs, levels = c("TREG", "CD4+T"))
plot(roc_curve_rf_pca, col = "red", main = "ROC Curve for Random Forest with PCA")

# Calculate AUC
auc_value_rf_pca <- auc(roc_curve_rf_pca)
cat("AUC (Random Forest with PCA):", round(auc_value_rf_pca, 3), "\n")

```

```{r}
cat("Random Forest with PCA Evaluation Summary:\n")
cat("Accuracy:", round(accuracy_rf_pca, 3), "\n")
cat("Balanced Accuracy:", round(balanced_accuracy_rf_pca, 3), "\n")
cat("F1 Score:", round(f1_score_rf_pca, 3), "\n")
cat("AUC:", round(auc_value_rf_pca, 3), "\n")

```

### K-NN with PCA

```{r}
# Create matrices for predictors (X)
train_X_pca <- as.matrix(train_pca[, -1])  # Exclude the label column
test_X_pca <- as.matrix(test_pca[, -1])

# Create vectors for labels (Y)
train_Y_pca <- train_pca$label
test_Y_pca <- test_pca$label

```

```{r}
# Train k-NN model with probabilities using caret's knn3
k <- 5  # Set the value of k
knn_model_pca <- knn3(train_X_pca, train_Y_pca, k = k)

# Predict probabilities and classes for the test set
knn_probabilities_pca <- predict(knn_model_pca, test_X_pca, type = "prob")
knn_pca_predictions <- predict(knn_model_pca, test_X_pca, type = "class")

```

# Reason we are using knn3 here:

The knn function from the class package works for simple k-NN because it directly predicts class labels based on the nearest neighbors. However, when we apply PCA, we need class probabilities to compute metrics like AUC and plot the ROC curve. The knn function doesn't support probability predictions, which is why it doesn't work for k-NN with PCA when AUC is required.

In contrast, knn3 from the caret package supports both class predictions and probability outputs, making it suitable for PCA-based k-NN with advanced metrics like AUC.

```{r}
# Confusion matrix
conf_matrix_knn_pca <- confusionMatrix(knn_pca_predictions, as.factor(test_Y_pca))
print(conf_matrix_knn_pca)

```

```{r}
# Extract specific metrics
accuracy_knn_pca <- conf_matrix_knn_pca$overall['Accuracy']
precision_knn_pca <- conf_matrix_knn_pca$byClass['Precision']
recall_knn_pca <- conf_matrix_knn_pca$byClass['Recall']
sensitivity_knn_pca <- conf_matrix_knn_pca$byClass['Sensitivity']
specificity_knn_pca <- conf_matrix_knn_pca$byClass['Specificity']

# Calculate F1 Score and Balanced Accuracy
f1_score_knn_pca <- 2 * ((precision_knn_pca * recall_knn_pca) / (precision_knn_pca + recall_knn_pca))
balanced_accuracy_knn_pca <- (sensitivity_knn_pca + specificity_knn_pca) / 2

# Print metrics
cat("Accuracy (k-NN with PCA):", round(accuracy_knn_pca, 3), "\n")
cat("Balanced Accuracy (k-NN with PCA):", round(balanced_accuracy_knn_pca, 3), "\n")
cat("F1 Score (k-NN with PCA):", round(f1_score_knn_pca, 3), "\n")

```

```{r}
# Convert labels to numeric for ROC calculation
test_Y_numeric_pca <- ifelse(test_Y_pca == "TREG", 1, 0)

# Extract probabilities for the positive class ("TREG")
knn_probs_treg <- knn_probabilities_pca[, "TREG"]



# Calculate and plot the ROC curve
roc_curve_knn_pca <- roc(test_Y_numeric_pca, knn_probs_treg)
auc_value_knn_pca <- auc(roc_curve_knn_pca)

# Plot ROC curve
plot(roc_curve_knn_pca, col = "red", main = "ROC Curve for k-NN with PCA")
cat("AUC (k-NN with PCA):", round(auc_value_knn_pca, 3), "\n")


```

```{r}
cat("k-NN with PCA Classifier Summary:\n")
cat("Accuracy:", round(accuracy_knn_pca, 3), "\n")
cat("Balanced Accuracy:", round(balanced_accuracy_knn_pca, 3), "\n")
cat("F1 Score:", round(f1_score_knn_pca, 3), "\n")
cat("AUC:", round(auc_value_knn_pca, 3), "\n")

```

### SVM with QDA

```{r}
# Train the SVM model on PCA-transformed data
svm_model_pca <- svm(label ~ ., data = train_pca, probability = TRUE)

# Print the SVM model summary
summary(svm_model_pca)
```

```{r}
# Predict class labels
svm_pca_predictions <- predict(svm_model_pca, test_pca, probability = TRUE)

# Predict probabilities
svm_pca_probs <- attr(predict(svm_model_pca, test_pca, probability = TRUE), "probabilities")

```

```{r}
# Confusion matrix
conf_matrix_svm_pca <- confusionMatrix(svm_pca_predictions, as.factor(test_pca$label))
print(conf_matrix_svm_pca)
```

```{r}
# Extract specific metrics
accuracy_svm_pca <- conf_matrix_svm_pca$overall['Accuracy']
precision_svm_pca <- conf_matrix_svm_pca$byClass['Precision']
recall_svm_pca <- conf_matrix_svm_pca$byClass['Recall']
sensitivity_svm_pca <- conf_matrix_svm_pca$byClass['Sensitivity']
specificity_svm_pca <- conf_matrix_svm_pca$byClass['Specificity']

# Calculate F1 Score and Balanced Accuracy
f1_score_svm_pca <- 2 * ((precision_svm_pca * recall_svm_pca) / (precision_svm_pca + recall_svm_pca))
balanced_accuracy_svm_pca <- (sensitivity_svm_pca + specificity_svm_pca) / 2

# Print metrics
cat("Accuracy (SVM with PCA):", round(accuracy_svm_pca, 3), "\n")
cat("Balanced Accuracy (SVM with PCA):", round(balanced_accuracy_svm_pca, 3), "\n")
cat("F1 Score (SVM with PCA):", round(f1_score_svm_pca, 3), "\n")

```

```{r}
# Convert labels to numeric for ROC calculation
test_Y_numeric_pca <- ifelse(test_pca$label == "TREG", 1, 0)

# Extract probabilities for the positive class ("TREG")
svm_probs_treg <- svm_pca_probs[, "TREG"]

# Calculate and plot the ROC curve
roc_curve_svm_pca <- roc(test_Y_numeric_pca, svm_probs_treg)
auc_value_svm_pca <- auc(roc_curve_svm_pca)

# Plot ROC curve
plot(roc_curve_svm_pca, col = "red", main = "ROC Curve for SVM with PCA")
cat("AUC (SVM with PCA):", round(auc_value_svm_pca, 3), "\n")
```

```{r}
cat("SVM with PCA Classifier Summary:\n")
cat("Accuracy:", round(accuracy_svm_pca, 3), "\n")
cat("Balanced Accuracy:", round(balanced_accuracy_svm_pca, 3), "\n")
cat("F1 Score:", round(f1_score_svm_pca, 3), "\n")
cat("AUC:", round(auc_value_svm_pca, 3), "\n")

```

### GBDT with PCA

```{r}

# Convert labels to numeric (required for GBDT)
train_pca$label <- as.numeric(as.factor(train_pca$label)) - 1
test_pca$label <- as.numeric(as.factor(test_pca$label)) - 1

library(gbm)

# Train GBDT model on PCA-transformed data
gbm_pca_model <- gbm(label ~ ., data = train_pca,
                     distribution = "bernoulli",
                     n.trees = 150,
                     interaction.depth = 5,
                     cv.folds = 5,
                     shrinkage = 0.1,
                     n.minobsinnode = 20,
                     verbose = FALSE)

# Determine the optimal number of trees
best_trees_pca <- gbm.perf(gbm_pca_model, method = "cv")

```

```{r}
# Predict probabilities and class labels
gbdt_pca_predictions <- predict(gbm_pca_model, test_pca[,-1], n.trees = best_trees_pca, type = "response")
gbdt_pca_predictions_class <- ifelse(gbdt_pca_predictions > 0.5, 1, 0)

```

```{r}
# Confusion matrix
gbdt_pca_conf_matrix <- confusionMatrix(as.factor(gbdt_pca_predictions_class), as.factor(test_pca$label))
print(gbdt_pca_conf_matrix)

# Extract metrics
gbdt_pca_accuracy <- mean(gbdt_pca_predictions_class == test_pca$label)
gbdt_pca_balanced_accuracy <- mean(c(gbdt_pca_conf_matrix$byClass["Sensitivity"], gbdt_pca_conf_matrix$byClass["Specificity"]))
gbdt_pca_f1_score <- 2 * (gbdt_pca_conf_matrix$byClass["Precision"] * gbdt_pca_conf_matrix$byClass["Recall"]) / 
  (gbdt_pca_conf_matrix$byClass["Precision"] + gbdt_pca_conf_matrix$byClass["Recall"])

# Print metrics
cat("Accuracy (GBDT with PCA):", round(gbdt_pca_accuracy, 3), "\n")
cat("Balanced Accuracy (GBDT with PCA):", round(gbdt_pca_balanced_accuracy, 3), "\n")
cat("F1 Score (GBDT with PCA):", round(gbdt_pca_f1_score, 3), "\n")
```

```{r}
# Calculate and plot the ROC curve
gbdt_pca_roc_curve <- roc(test_pca$label, gbdt_pca_predictions)
gbdt_pca_auc_value <- auc(gbdt_pca_roc_curve)

# Plot ROC curve
plot.roc(gbdt_pca_roc_curve, main = "GBDT ROC Curve with PCA", col = "red", lwd = 2)
cat("AUC (GBDT with PCA):", round(gbdt_pca_auc_value, 3), "\n")
```

```{r}
cat("GBDT with PCA Classifier Summary:\n")
cat("Accuracy:", round(gbdt_pca_accuracy, 3), "\n")
cat("Balanced Accuracy:", round(gbdt_pca_balanced_accuracy, 3), "\n")
cat("F1 Score:", round(gbdt_pca_f1_score, 3), "\n")
cat("AUC:", round(gbdt_pca_auc_value, 3), "\n")

```

## T1.3 Training and evaluation of classifiers

Below, we train and evaluate three classifiers with the objective of improving the F1 score.

## 1. Logistic Regression with L2 Regularization (Ridge Regression)

L2 regularization, also known as Ridge Regression, is a technique used to prevent overfitting in machine learning models. It works by adding a penalty term to the model's loss function:

Loss = Original loss + λ Σ βi^2

Here, βi represents the model coefficients, and λ is a regularization parameter that controls the strength of the penalty. This additional term discourages large coefficients by shrinking them closer to zero, effectively reducing model complexity.

L2 regularization prevents overfitting by penalizing large coefficients, improving generalization and robustness in high-dimensional datasets with correlated features. It also balances bias and variance, ensuring better performance on unseen data.

In this project, L2 regularization was applied to logistic regression to achieve a more robust model and compare its performance against the standard logistic regression model. The results demonstrated improved generalization with a better balance between precision and recall, as reflected in the higher F1 score and AUC.

```{r}
library(glmnet)

# Convert data to matrix format for glmnet
train_X_log <- as.matrix(train_data_log[, -1])  
test_X_log <- as.matrix(test_data_log[, -1])   
train_Y_log <- train_data_log$label
test_Y_log <- test_data_log$label

```


```{r}
# Train logistic regression with L2 regularization (alpha = 0)
logistic_l2_model <- glmnet(train_X_log, train_Y_log, family = "binomial", alpha = 0)

# Cross-validate to find the optimal lambda (regularization parameter)
cv_model <- cv.glmnet(train_X_log, train_Y_log, family = "binomial", alpha = 0)
best_lambda <- cv_model$lambda.min

# Train the final model using the best lambda
final_l2_model <- glmnet(train_X_log, train_Y_log, family = "binomial", alpha = 0, lambda = best_lambda)

```


```{r}
# Predict probabilities on the test set
logistic_l2_predictions <- predict(final_l2_model, newx = test_X_log, s = best_lambda, type = "response")

# Convert probabilities to class labels (threshold = 0.5)
logistic_l2_predicted_labels <- ifelse(logistic_l2_predictions > 0.5, 1, 0)
```


```{r}
library(caret)

# Confusion matrix
logistic_l2_conf_matrix <- confusionMatrix(as.factor(logistic_l2_predicted_labels), as.factor(test_Y_log))
print(logistic_l2_conf_matrix)

# Extract metrics
accuracy_logistic_l2 <- logistic_l2_conf_matrix$overall['Accuracy']
balanced_accuracy_logistic_l2 <- logistic_l2_conf_matrix$byClass['Balanced Accuracy']
f1_score_logistic_l2 <- logistic_l2_conf_matrix$byClass['F1']
```

```{r}
library(pROC)

# Calculate and plot the ROC curve
roc_curve_logistic_l2 <- roc(test_Y_log, as.numeric(logistic_l2_predictions))  # Use probabilities for ROC
plot(roc_curve_logistic_l2, main = "ROC Curve for Logistic Regression with L2", col='red')
auc_logistic_l2 <- auc(roc_curve_logistic_l2)

```

```{r}
cat("Logistic Regression with L2 Regularization Evaluation Summary:\n")
cat("Accuracy:", round(accuracy_logistic_l2, 3), "\n")
cat("Balanced Accuracy:", round(balanced_accuracy_logistic_l2, 3), "\n")
cat("F1 Score:", round(f1_score_logistic_l2, 3), "\n")
cat("AUC:", round(auc_logistic_l2, 3), "\n")
```

The F1 score of 0.947 achieved with Logistic Regression using L2 Regularization indicates that the model performs exceptionally well in classifying the data, with a strong balance between false positives and false negatives. While slightly lower than other approaches such as Elastic Net or the meta-classifier, this result still reflects a highly effective model with minimal compromise on precision or recall. This performance makes L2-regularized logistic regression a competitive choice for robust classification.


## 2. Elastic Net Regularization (Combination of L1 and L2)

Elastic Net Regularization combines the strengths of L1 (Lasso) and L2 (Ridge) penalties to balance feature selection and model stability. It adds a penalty to the loss function that promotes sparsity (via L1) while preventing overfitting (via L2), making it effective in handling correlated predictors and high-dimensional datasets. Elastic Net is particularly useful when selecting groups of related features while maintaining robust generalization.

```{r}
library(glmnet)

# Prepare data for glmnet
train_X <- as.matrix(train_data1[, -1])
test_X <- as.matrix(test_data1[, -1])
train_Y <- as.numeric(as.factor(train_data1$label)) - 1
test_Y <- as.numeric(as.factor(test_data1$label)) - 1
```


```{r}
# Train Elastic Net (alpha = 0.5 combines L1 and L2)
elastic_net_model <- cv.glmnet(train_X, train_Y, alpha = 0.5, family = "binomial")

# Best lambda
best_lambda <- elastic_net_model$lambda.min

# Final model
final_elastic_net_model <- glmnet(train_X, train_Y, alpha = 0.5, lambda = best_lambda, family = "binomial")

# Predictions
elastic_net_predictions <- predict(final_elastic_net_model, newx = test_X, s = best_lambda, type = "response")
elastic_net_predicted_labels <- ifelse(elastic_net_predictions > 0.5, 1, 0)

# Evaluate
conf_matrix_regularization <- confusionMatrix(as.factor(elastic_net_predicted_labels), as.factor(test_Y))
# print(conf_matrix_regularization)
cat("F1 Score (Regularization - Elastic Net):", round(conf_matrix_regularization$byClass['F1'], 3), "\n")
```

The F1 score of 0.966 achieved with Elastic Net Regularization demonstrates excellent model performance, balancing precision and recall effectively. This high F1 score indicates the model's robustness in minimizing both false positives and false negatives. By combining L1 (feature selection) and L2 (stabilization) penalties, Elastic Net has likely improved the model's ability to generalize to unseen data while selecting the most relevant features. This makes it particularly effective in handling any multicollinearity or irrelevant predictors present in the dataset. Overall, the result highlights the strength of Elastic Net in creating a well-regularized, high-performing model.

```{r}
library(pROC)

# Calculate the ROC curve
roc_curve_elastic_net <- roc(test_Y, elastic_net_predictions)

# Plot the ROC curve
plot(roc_curve_elastic_net, main = "ROC Curve for Elastic Net Regularization", col = "blue", lwd = 2)

# Calculate the AUC
auc_elastic_net <- auc(roc_curve_elastic_net)
cat("AUC (Elastic Net Regularization):", round(auc_elastic_net, 3), "\n")

```

## 3. Meta Classifier

The meta-classifier combines the strengths of multiple base classifiers (LDA, Random Forest, and GBDT) to achieve better performance. By learning how to aggregate predictions, it can handle situations where individual classifiers might struggle, leading to improved metrics such as the F1 score. The evaluation demonstrates the effectiveness of this ensemble learning approach in combining diverse classifiers.

```{r}
# Train base classifiers
lda_pred <- predict(lda_model, test_data1)$class
rf_pred <- predict(rf_model, test_data1)
gbdt_pred <- ifelse(gbdt_predictions > 0.5, 1, 0)

# Combine predictions into a new dataset
meta_features <- data.frame(lda = as.numeric(lda_pred), 
                            rf = as.numeric(rf_pred), 
                            gbdt = gbdt_pred, 
                            label = as.numeric(as.factor(test_data1$label)) - 1)  # Convert labels to numeric (e.g., 0 and 1)

# Train a meta-classifier (e.g., Logistic Regression)
meta_model <- glm(label ~ ., data = meta_features, family = binomial)

# Predict probabilities on the meta features
meta_predictions <- predict(meta_model, newdata = meta_features, type = "response")

# Convert probabilities to class labels
meta_predicted_labels <- ifelse(meta_predictions > 0.5, 1, 0)

# Convert predicted and true labels to factors with matching levels
meta_predicted_labels <- factor(meta_predicted_labels, levels = c(0, 1))
meta_features$label <- factor(meta_features$label, levels = c(0, 1))

```

```{r}
# Compute confusion matrix
library(caret)
conf_matrix_meta <- confusionMatrix(meta_predicted_labels, meta_features$label)
print(conf_matrix_meta)

# Extract metrics
accuracy_meta <- conf_matrix_meta$overall['Accuracy']
balanced_accuracy_meta <- conf_matrix_meta$byClass['Balanced Accuracy']
f1_score_meta <- conf_matrix_meta$byClass['F1']

# Print the metrics
cat("Accuracy (Meta-Classifier):", round(accuracy_meta, 3), "\n")
cat("Balanced Accuracy (Meta-Classifier):", round(balanced_accuracy_meta, 3), "\n")
cat("F1 Score (Meta-Classifier):", round(f1_score_meta, 3), "\n")

```

The results demonstrate that the meta-classifier effectively combines the predictions of the base classifiers (LDA, Random Forest, and GBDT) to achieve outstanding performance. The high F1 score (96.4%) shows that the meta-classifier excels in balancing precision and recall, making it highly suitable for applications where both false positives and false negatives are critical. This performance improvement justifies the use of ensemble learning in this context.


```{r}
library(pROC)

# Calculate the ROC curve
roc_curve_meta <- roc(as.numeric(as.character(meta_features$label)), as.numeric(meta_predictions))

# Plot the ROC curve
plot(roc_curve_meta, main = "ROC Curve for Meta-Classifier", col = "darkgreen", lwd = 2)

# Calculate the AUC
auc_meta <- auc(roc_curve_meta)
cat("AUC (Meta-Classifier):", round(auc_meta, 3), "\n")

```

