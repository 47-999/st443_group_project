---
title: "ST443 Group Project - Task 1"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

# ST443 Group Project

# Task 1: Binary classification

## Introduction

The aim of Task 1 is to classify immune cell types based on RNA expression levels.

Load and View data:

```{r}
#setwd("C:/Users/nagel freya/OneDrive - The Boston Consulting Group, Inc/Documents/01 Master/01 Uni/01 Term/ST443 Machine Learning and Data Mining/00 Group Project")
RNAdata <-read.csv("data1.csv.gz", header=TRUE)
View(RNAdata)
```

Check for missing data:

```{r}
any(is.na(RNAdata))
```

No data is missing.

Check for infinite values

```{r}
sum(sapply(RNAdata, function(x) sum(is.infinite(x))))
```

No infinite values.

## T1.1 Data preparation and Summary Statistics

### Balance / share of different celltypes in dataset

We first create a new column that assigns each celltype (CD4+T and TREG) a numerical value, such that we can calculate statistics that are interesting to us with respect to each different celltype.

```{r}
RNAdata$label <- as.numeric(factor(RNAdata$label))
table(RNAdata$label)
table(RNAdata$label) / nrow(RNAdata)
```

We find that the dataset contains 3356 (\~61.34%) CD4+T cells (numerical value = 1) and 2115 (38.65%) TREG cells (numerical value = 2).

To visualize this distribution, we generated a bar plot.

```{r}
# Load ggplot2 library for plotting
library(ggplot2)

# Create a table of class counts
class_counts <- table(RNAdata$label)

# Convert the table to a data frame for ggplot
class_counts_df <- as.data.frame(class_counts)
colnames(class_counts_df) <- c("Label", "Count")

# Bar plot for class distribution
ggplot(class_counts_df, aes(x = Label, y = Count, fill = Label)) +
  geom_bar(stat = "identity") +
  labs(title = "Class Distribution", x = "Cell Type", y = "Count") +
  theme_minimal()

```

As seen in the plot, there is a class imbalance, with CD4+T cells being more prevalent than TREG cells. This imbalance could potentially impact the performance of machine learning models, as classifiers might be biased toward the majority class.

### Significance of genes for different celltypes

Now we generate a new dataset that contains the mean and variance of each gene's expression across cells and for each celltype individually. This will help us later understand the significance of certain gene expression levels for the different celltypes.

```{r}
gene_stats <- data.frame(
  Mean = colMeans(RNAdata[,-1], na.rm = TRUE),
  Variance = apply(RNAdata[,-1], 2, var, na.rm = TRUE),
# Subset for CD4+T cells
  Mean_CD4 = colMeans(RNAdata[RNAdata$label == 1, -1], na.rm = TRUE),
  Variance_CD4 = apply(RNAdata[RNAdata$label == 2, -1], 2, var, na.rm = TRUE),
  # Subset for TREG cells
  Mean_TREG = colMeans(RNAdata[RNAdata$label == 1, -1], na.rm = TRUE),
  Variance_TREG = apply(RNAdata[RNAdata$label == 2, -1], 2, var, na.rm = TRUE)
)
View(gene_stats)
```

We also generate a dataset that contains the minimum and the maximum value of each gene.

```{r}
# Calculate the minimum for each gene
min_values <- apply(RNAdata[,-1], 2, min)

# Calculate the maximum for each gene
max_values <- apply(RNAdata[,-1], 2, max)

# Combine the results into a data frame
min_max_values <- data.frame(
  Gene = colnames(RNAdata[,-1]),  # Gene names
  Minimum = min_values,                 # Minimum values
  Maximum = max_values                  # Maximum values
)


View(min_max_values)
```

### Understand Gene Expression patterns

To understand the Gene Expression patterns we create histograms for a few genes. To understand whether the gene expression patterns are inherently different for the two celltypes, we use the subsets of data for each celltype and compare the histograms.

```{r}
# Create subsets
CD4_data <- subset(RNAdata, label == 1)
TREG_data <- subset(RNAdata, label == 2)

# Pick 4 genes at random
gene_columns <- colnames(RNAdata)[-1]
random_genes <- sample(gene_columns, 4)

par(mfrow = c(4, 3), mar = c(4, 4, 2, 1)) 

for (gene in random_genes) {
  
  # Histogram for gene over all celltypes
  hist(RNAdata[[gene]],
       main = paste("All Cells -", gene),
       xlab = "Expression",
       col = "blue",
       breaks = 20)
  
  # Histogram for gene over CD4+T cells
  hist(CD4_data[[gene]],
       main = paste("CD4+T -", gene),
       xlab = "Expression",
       col = "green",
       breaks = 20)

  # Histogram for gene over TREG cells
  hist(TREG_data[[gene]],
       main = paste("TREG - ", gene),
       xlab = "Expression",
       col = "yellow",
       breaks = 20)
}
```

We can see that there is no significant difference in the distribution of the expressed genes between CD4+T and TREG cells. We can further see that across all celltypes and genes, there is a high number of cells without expression of that specific gene (expression = 0). For the rest of the cells, the expression follows a normal distribution around some mean between 2 and 5.

# Density plots for genes

The code below generates density plots for the expression levels of randomly selected genes. This helps visualize the distribution of RNA expression for each gene across the two cell types (CD4+T and TREG).

```{r}
# Randomly select 5 genes for visualization
set.seed(Sys.time())  # Set a random seed for reproducibility
selected_genes <- sample(colnames(RNAdata[,-1]), 5)

# Loop through the selected genes and create density plots
for (gene in selected_genes) {
  p <- ggplot(RNAdata, aes(x = .data[[gene]], fill = label)) +  # Use full RNAdata with `label`
    geom_density(alpha = 0.5) +
    labs(title = paste("Density Plot for Gene:", gene), x = "Expression Level", y = "Density") +
    theme_minimal()
  
  print(p)  # Explicitly print each plot
}
```

The density plots for most genes reveal highly overlapping distributions for CD4+T and TREG cells, suggesting that many genes do not exhibit significant differential expression between the two cell types. This overlap implies that these genes may not be strong discriminators for classification and might be less relevant for building effective predictive models. Such observations underscore the importance of identifying key genes or features that show greater separation between the classes, as they are likely to contribute more meaningfully to the task of binary classification.

# Scatter plots

The scatter plots also helps us visualize the expression levels of randomly selected pairs of genes, comparing their distributions between the two cell types (CD4+T and TREG)

```{r}
# Randomly select 2 pairs of genes for scatter plots
set.seed(Sys.time())
selected_genes <- sample(colnames(RNAdata[,-1]), 4)

# Create scatter plots for the selected pairs
library(ggplot2)
ggplot(RNAdata, aes(x = .data[[selected_genes[1]]], y = .data[[selected_genes[2]]], color = label)) +
  geom_point(alpha = 0.7) +
  labs(title = paste("Scatter Plot:", selected_genes[1], "vs", selected_genes[2]), 
       x = selected_genes[1], y = selected_genes[2]) +
  theme_minimal()

ggplot(RNAdata, aes(x = .data[[selected_genes[3]]], y = .data[[selected_genes[4]]], color = label)) +
  geom_point(alpha = 0.7) +
  labs(title = paste("Scatter Plot:", selected_genes[3], "vs", selected_genes[4]), 
       x = selected_genes[3], y = selected_genes[4]) +
  theme_minimal()

```

The significant overlap in distributions between CD4+T and TREG cells indicates that these gene pairs may not be highly informative for distinguishing the two classes. This highlights the importance of identifying gene pairs or combinations with stronger class separation for effective feature selection and classification.

### Sparsity of Data

RNA data is often sparse, i.e. has a significant number of null-values - our histogram plots above already indicate this. We calculate the sparsity of our dataframe by dividing the number of zero entries by the total number of entries.

```{r}
sum(RNAdata == 0)/(dim(RNAdata)[1]*dim(RNAdata)[2])
```

We get that 66.17% of our entries are nullvalues, meaning that our data is very sparse and thus less variant, as most entries are repeated.

## T1.2 Training and evaluation of classifiers

### Split data into Train and Test data

```{r}
# Loading necessary libraries
library(MASS)
library(caret)
library(pROC)

# Set seed for reproducibility
set.seed(123)  

# Create training (80%) and testing (20%) sets
sample_index <- createDataPartition(RNAdata$label, p = 0.8, list = FALSE)
train_data1 <- RNAdata[sample_index, ]
test_data1 <- RNAdata[-sample_index, ]
```

### Linear Discriminant Analysis (LDA)

```{r}
# Train the LDA model
lda_model <- lda(label ~ ., data = train_data1)
print(lda_model)
```

```{r}
# Make predictions on the test data
lda_predictions <- predict(lda_model, test_data1[,-1])
lda_predicted_labels <- lda_predictions$class
```

```{r}
# Confusion matrix
lda_conf_matrix <- confusionMatrix(lda_predicted_labels, as.factor(test_data1$label))
lda_conf_matrix
```

```{r}
# Extracting specific metrics
lda_accuracy <- conf_matrix$overall['Accuracy']
lda_balanced_accuracy <- conf_matrix$byClass['Balanced Accuracy']
lda_f1_score <- conf_matrix$byClass['F1']
```

```{r}
cat("Accuracy:", lda_accuracy, "\n")
cat("Balanced Accuracy:", lda_balanced_accuracy, "\n")
cat("F1 Score:", lda_f1_score, "\n")
```

```{r}
# Convert labels to numeric for ROC curve calculation
lda_probs <- lda_predictions$posterior[, 2]

# Create ROC curve
lda_roc_curve <- roc(as.factor(test_data1$label), lda_probs)
plot(lda_roc_curve, col = "pink", main = "ROC Curve for LDA")
lda_auc_value <- auc(lda_roc_curve)
cat("AUC:", lda_auc_value, "\n")
```

```{r}
cat("LDA Model Evaluation Summary:\n")
cat("Accuracy:", round(lda_accuracy, 3), "\n")
cat("Balanced Accuracy:", round(lda_balanced_accuracy, 3), "\n")
cat("F1 Score:", round(lda_f1_score, 3), "\n")
cat("AUC:", round(lda_auc_value, 3), "\n")
```

### Logistic Classifier

```{r}
# Train a logistic regression model
logistic_model <- glm(label ~ ., data = train_data1, family = binomial)

# Print the model summary
summary(logistic_model)
```

```{r}
# Predict probabilities on the test set
logistic_predictions <- predict(logistic_model, test_data1[, -1], type = "response")

# Convert probabilities to class labels (threshold = 0.5)
logistic_predicted_labels <- ifelse(logistic_predictions > 0.5, 2, 1)
```

```{r}
# Calculate test error
logistic_test_error <- mean(logistic_predicted_labels != test_data1$label)

# Print the test error
cat("Test Error (Logistic):", round(logistic_test_error, 3), "\n")
```

```{r}
# Confusion matrix
logistic_conf_matrix <- confusionMatrix(as.factor(logistic_predicted_labels), as.factor(test_data1$label))
print(logistic_conf_matrix)
```

```{r}
# Extract metrics from the confusion matrix
accuracy_logistic <- logistic_conf_matrix$overall['Accuracy']
balanced_accuracy_logistic <- logistic_conf_matrix$byClass['Balanced Accuracy']
f1_score_logistic <- logistic_conf_matrix$byClass['F1']

# Print the metrics
cat("Accuracy (Logistic):", accuracy_logistic, "\n")
cat("Balanced Accuracy (Logistic):", balanced_accuracy_logistic, "\n")
cat("F1 Score (Logistic):", f1_score_logistic, "\n")
```

```{r}
# Calculate and plot the ROC curve
roc_curve_logistic <- roc(test_data1$label, logistic_predictions)  # Use probabilities for ROC
plot(roc_curve_logistic, main = "ROC Curve for Logistic Regression", col='red')
auc_logistic <- auc(roc_curve_logistic)
cat("AUC:", auc_logistic, "\n")
```

Logistic Regression Model Evaluation Summary:

```{r}
cat("Logistic Regression Model Evaluation Summary:\n")
cat("Accuracy:", round(accuracy_logistic, 3), "\n")
cat("Balanced Accuracy:", round(balanced_accuracy_logistic, 3), "\n")
cat("F1 Score:", round(f1_score_logistic, 3), "\n")
cat("AUC:", round(auc_logistic, 3), "\n")
```

### Quadratic Discriminant Analysis (QDA)

```{r}
library(klaR)
# Train a QDA model (using rda from klaR library to account for the high-dimensional data by introducing a small regularization factor to the covariance matrices, making them invertible even with the few samples available)
qda_model <- rda(label ~ ., data = train_data1, gamma = 0.01, lambda = 0.5)

# Print the model summary
summary(qda_model)
```

```{r}
# Predict the label of the test data
qda_predictions <- predict(qda_model, test_data1[,-1])  # Exclude label column
qda_predicted_labels <- qda_predictions$class
```

```{r}
# Confusion matrix
qda_conf_matrix <- confusionMatrix(qda_predicted_labels, as.factor(test_data1$label))
qda_conf_matrix
```

```{r}
# Extract metrics from the confusion matrix
qda_accuracy <- qda_conf_matrix$overall['Accuracy']
qda_balanced_accuracy <- qda_conf_matrix$byClass['Balanced Accuracy']
qda_f1_score <- qda_conf_matrix$byClass['F1']

# Print the metrics
cat("Accuracy (QDA):", qda_accuracy, "\n")
cat("Balanced Accuracy (QDA):", qda_balanced_accuracy, "\n")
cat("F1 Score (QDA):", qda_f1_score, "\n")
```

```{r}
# Calculate and plot the ROC curve
roc_curve_qda <- roc(test_data1$label, qda_predictions)  # Use probabilities for ROC
plot(roc_curve_qda, main = "ROC Curve for QDA", col='red')
auc_qda <- auc(roc_curve_qda)
cat("AUC:", auc_qda, "\n")
```

```{r}
cat("QDA Model Evaluation Summary:\n")
cat("Accuracy:", round(qda_accuracy, 3), "\n")
cat("Balanced Accuracy:", round(qda_balanced_accuracy, 3), "\n")
cat("F1 Score:", round(qda_f1_score, 3), "\n")
cat("AUC:", round(qda_auc_value, 3), "\n")
```

### Random Forest

```{r}
library(randomForest)
```

```{r}
# Train Random Forest with mtry = sqrt(number of predictors)
p <- ncol(train_data1) - 1  # Number of predictors (excluding the label column)
train_data1$label <- as.factor(train_data1$label)
rf_model <- randomForest(label ~ ., data = train_data1, mtry = round(sqrt(p)), ntree = 500, importance = TRUE)

# Print the model summary
print(rf_model)
```

```{r}
# Predictions on the test data
rf_predictions <- predict(rf_model, test_data1)

# Evaluating the model with a confusion matrix
conf_matrix_rf <- confusionMatrix(rf_predictions, as.factor(test_data1$label))
print(conf_matrix_rf)
```

```{r}
# Extracting specific performance metrics
accuracy_rf <- conf_matrix_rf$overall['Accuracy']
balanced_accuracy_rf <- conf_matrix_rf$byClass['Balanced Accuracy']
f1_score_rf <- conf_matrix_rf$byClass['F1']

cat("Accuracy:", accuracy_rf, "\n")
cat("Balanced Accuracy:", balanced_accuracy_rf, "\n")
cat("F1 Score:", f1_score_rf, "\n")
```

```{r}
# Getting predicted probabilities for "TREG" class
rf_probs <- predict(rf_model, test_data1, type = "prob")[, 2]
```

```{r}
# Create ROC curve
library(pROC)
roc_curve_rf <- roc(test_data1$label, rf_probs)
plot(roc_curve_rf, col = "blue", main = "ROC Curve for Random Forest")
```

```{r}
#calculating AUC
auc_value_rf <- auc(roc_curve_rf)
cat("AUC:", auc_value_rf, "\n")
```

```{r}
cat("Random Forest Evaluation Summary:\n")
cat("Accuracy:", round(accuracy_rf, 3), "\n")
cat("Balanced Accuracy:", round(balanced_accuracy_rf, 3), "\n")
cat("F1 Score:", round(f1_score_rf, 3), "\n")
cat("AUC:", round(auc_value_rf, 3), "\n")
```

### K-NN

```{r}
library(class)
```

```{r}
# Create matrices for predictors (X)
train_X <- as.matrix(train_data1[, -1])  # Exclude the label column
test_X <- as.matrix(test_data1[, -1])

# Create vectors for labels (Y)
train_Y <- train_data1$label
test_Y <- test_data1$label
```

```{r}
# Setting k = 5
k <- 5

# Perform k-NN classification
knn_predictions <- knn(train_X, test_X, train_Y, k = k)
```

```{r}
# Confusion matrix
conf_matrix_knn <- confusionMatrix(knn_predictions, as.factor(test_Y))
print(conf_matrix_knn)

# Extract the confusion matrix as a table
confusion_table <- conf_matrix_knn$table
print(confusion_table)

#Calculating Accuracy
accuracy_knn <- sum(diag(confusion_table)) / sum(confusion_table)
cat("Accuracy:", accuracy_knn, "\n")

```

```{r}
precision_knn <- conf_matrix_knn$byClass["Precision"]
recall_knn <- conf_matrix_knn$byClass["Recall"]
f1_score_knn <- 2 * ((precision_knn * recall_knn) / (precision_knn + recall_knn))
cat("F1 Score:", f1_score_knn, "\n")
```

```{r}
sensitivity_knn <- conf_matrix_knn$byClass["Sensitivity"]
specificity_knn <- conf_matrix_knn$byClass["Specificity"]
balanced_accuracy_knn <- (sensitivity_knn + specificity_knn) / 2
cat("Balanced Accuracy:", balanced_accuracy_knn, "\n")
```

```{r}
# Calculate pseudo-probabilities for AUC
k <- 5  #The same k as used above
knn_probabilities <- attr(knn_predictions, "prob")
knn_probabilities <- ifelse(knn_predictions == "TREG", knn_probabilities, 1 - knn_probabilities)
summary(knn_probabilities)  # Ensure values range between 0 and 1
```

```{r}
# Check levels of the test_Y factor
levels(test_Y)
table(test_Y)
```

```{r}
test_Y_numeric <- ifelse(test_Y == "TREG", 1, 0)  # Assuming "TREG" is the positive class
table(test_Y_numeric)
```

```{r}
# Calculate AUC
roc_curve_knn <- roc(test_Y_numeric, knn_probabilities)
auc_value_knn <- auc(roc_curve_knn)
cat("AUC:", auc_value_knn, "\n")
```

```{r}
# Plot ROC curve
plot(roc_curve_knn, col = "blue", main = "ROC Curve for k-NN")
```

```{r}
cat("Knn Classifiers Summary:\n")
cat("Accuracy:", round(accuracy_knn, 3), "\n")
cat("Balanced Accuracy:", round(balanced_accuracy_knn, 3), "\n")
cat("F1 Score:", round(f1_score_knn, 3), "\n")
cat("AUC:", round(auc_value_knn, 3), "\n")
```

### Support Vector Machine (SVM)

```{r}
#Packages
library(e1071)
```

```{r}
RNAdata$label <- factor(RNAdata$label)

#Model
svm_model <- svm(train_data1$label ~ ., data = train_data1, kernel = "linear")
svm_predictions <- predict(svm_model, test_data1[,-1])
```

```{r}
# Accuracy
svm_accuracy <- mean(svm_predictions == test_data1$label)
print(paste("SVM Accuracy: ", svm_accuracy))

# Confusion matrix
svm_conf_matrix <- table(Predicted = svm_predictions, Actual = test_data1$label)
print(svm_conf_matrix)

svm_recall_per_class <- diag(svm_conf_matrix) / rowSums(svm_conf_matrix)

# Balanced accuracy
svm_balanced_accuracy <- mean(svm_recall_per_class)
print(paste("SVM Balanced Accuracy: ", svm_balanced_accuracy))

svm_precision_per_class <- diag(svm_conf_matrix) / colSums(svm_conf_matrix)
svm_f1_score_per_class <- 2 * (svm_precision_per_class * svm_recall_per_class) / (svm_precision_per_class + svm_recall_per_class)

# F1 Scores for each class
cat("SVM F1 Score per class: ", svm_f1_score_per_class, "\n")
svm_average_f1_score <- mean(svm_f1_score_per_class)
print(paste("SVM Average F1-Score: ", svm_average_f1_score))

```

### Gradient Boosting Decision Tree (GBDT)

```{r}
#Packages
library(gbm)
library(caret)
library(pROC)
```

```{r}
RNAdata$label <- as.numeric(as.factor(RNAdata$label)) - 1

#Model
gbm_model <- gbm(label ~ ., data = train_data1, 
                 distribution = "bernoulli", 
                 n.trees = 100, 
                 interaction.depth = 4, 
                 cv.folds = 5, 
                 verbose = FALSE)

best_trees <- gbm.perf(gbm_model, method = "cv")

gbdt_predictions <- predict(gbm_model, test_data1[,-1], n.trees = best_trees, type = "response")

gbdt_predictions_class <- ifelse(gbdt_predictions > 0.5, 1, 0)
```

```{r}
# Accuracy
gbdt_accuracy <- mean(gbdt_predictions_class == test_data1$label)
print(paste("GBDT Accuracy: ", gbdt_accuracy))

# Confusion Matrix
gbdt_conf_matrix <- confusionMatrix(as.factor(gbdt_predictions_class), as.factor(test_data1$label))
print(gbdt_conf_matrix)

# Balanced Accuracy
gbdt_balanced_accuracy <- mean(gbdt_conf_matrix$byClass["Sensitivity"], gbdt_conf_matrix$byClass["Specificity"])
print(paste("GBDT Balanced Accuracy: ", gbdt_balanced_accuracy))

# F1 Score
gbdt_f1_score <- 2 * (gbdt_conf_matrix$byClass["Precision"] * gbdt_conf_matrix$byClass["Recall"]) / 
  (gbdt_conf_matrix$byClass["Precision"] + gbdt_conf_matrix$byClass["Recall"])
print(paste("GBDT F1 Score: ", gbdt_f1_score))

# AUC 
gbdt_roc_curve <- roc(test_data1$label, gbdt_predictions)
gbdt_auc_value <- auc(gbdt_roc_curve)
print(paste("GBDT AUC: ", gbdt_auc_value))

# ROC Curve
plot.roc(gbdt_roc_curve, main = "GBDT ROC Curve", col = "blue", lwd = 2)

```

## T1.3 Training and evaluation of classifiers
