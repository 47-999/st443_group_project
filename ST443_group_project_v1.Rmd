---
title: "ST443 Group Project - Task 1"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

# ST443 Group Project

# Task 1: Binary classification

## Introduction

The aim of Task 1 is to classify immune cell types based on RNA expression levels.

Load and View data:

```{r}
#setwd("C:/Users/nagel freya/OneDrive - The Boston Consulting Group, Inc/Documents/01 Master/01 Uni/01 Term/ST443 Machine Learning and Data Mining/00 Group Project")
RNAdata <-read.csv("data1.csv.gz", header=TRUE)
View(RNAdata)
```

Check for missing data:

```{r}
any(is.na(RNAdata))
```

No data is missing.

Check for infinite values

```{r}
sum(sapply(RNAdata, function(x) sum(is.infinite(x))))
```

No infinite values.

## T1.1 Data preparation and Summary Statistics

### Balance / share of different celltypes in dataset

We first create a new column that assigns each celltype (CD4+T and TREG) a numerical value, such that we can calculate statistics that are interesting to us with respect to each different celltype.

```{r}
RNAdata$label <- as.numeric(factor(RNAdata$label))
table(RNAdata$label)
table(RNAdata$label) / nrow(RNAdata)
```

We find that the dataset contains 3356 (\~61.34%) CD4+T cells (numerical value = 1) and 2115 (38.65%) TREG cells (numerical value = 2).

To visualize this distribution, we generated a bar plot.

```{r}
# Load ggplot2 library for plotting
library(ggplot2)

# Create a table of class counts
class_counts <- table(RNAdata$label)

# Convert the table to a data frame for ggplot
class_counts_df <- as.data.frame(class_counts)
colnames(class_counts_df) <- c("Label", "Count")

# Bar plot for class distribution
ggplot(class_counts_df, aes(x = Label, y = Count, fill = Label)) +
  geom_bar(stat = "identity") +
  labs(title = "Class Distribution", x = "Cell Type", y = "Count") +
  theme_minimal()

```

As seen in the plot, there is a class imbalance, with CD4+T cells being more prevalent than TREG cells. This imbalance could potentially impact the performance of machine learning models, as classifiers might be biased toward the majority class.

### Significance of genes for different celltypes

Now we generate a new dataset that contains the mean and variance of each gene's expression across cells and for each celltype individually. This will help us later understand the significance of certain gene expression levels for the different celltypes.

```{r}
gene_stats <- data.frame(
  Mean = colMeans(RNAdata[,-1], na.rm = TRUE),
  Variance = apply(RNAdata[,-1], 2, var, na.rm = TRUE),
# Subset for CD4+T cells
  Mean_CD4 = colMeans(RNAdata[RNAdata$label == 1, -1], na.rm = TRUE),
  Variance_CD4 = apply(RNAdata[RNAdata$label == 2, -1], 2, var, na.rm = TRUE),
  # Subset for TREG cells
  Mean_TREG = colMeans(RNAdata[RNAdata$label == 1, -1], na.rm = TRUE),
  Variance_TREG = apply(RNAdata[RNAdata$label == 2, -1], 2, var, na.rm = TRUE)
)
View(gene_stats)
```

We also generate a dataset that contains the minimum and the maximum value of each gene.

```{r}
# Calculate the minimum for each gene
min_values <- apply(RNAdata[,-1], 2, min)

# Calculate the maximum for each gene
max_values <- apply(RNAdata[,-1], 2, max)

# Combine the results into a data frame
min_max_values <- data.frame(
  Gene = colnames(RNAdata[,-1]),  # Gene names
  Minimum = min_values,                 # Minimum values
  Maximum = max_values                  # Maximum values
)


View(min_max_values)
```

### Understand Gene Expression patterns

To understand the Gene Expression patterns we create histograms for a few genes. To understand whether the gene expression patterns are inherently different for the two celltypes, we use the subsets of data for each celltype and compare the histograms.

```{r}
# Create subsets
CD4_data <- subset(RNAdata, label == 1)
TREG_data <- subset(RNAdata, label == 2)

# Pick 4 genes at random
gene_columns <- colnames(RNAdata)[-1]
random_genes <- sample(gene_columns, 4)

par(mfrow = c(4, 3), mar = c(4, 4, 2, 1)) 

for (gene in random_genes) {
  
  # Histogram for gene over all celltypes
  hist(RNAdata[[gene]],
       main = paste("All Cells -", gene),
       xlab = "Expression",
       col = "blue",
       breaks = 20)
  
  # Histogram for gene over CD4+T cells
  hist(CD4_data[[gene]],
       main = paste("CD4+T -", gene),
       xlab = "Expression",
       col = "green",
       breaks = 20)

  # Histogram for gene over TREG cells
  hist(TREG_data[[gene]],
       main = paste("TREG - ", gene),
       xlab = "Expression",
       col = "yellow",
       breaks = 20)
}
```

We can see that there is no significant difference in the distribution of the expressed genes between CD4+T and TREG cells. We can further see that across all celltypes and genes, there is a high number of cells without expression of that specific gene (expression = 0). For the rest of the cells, the expression follows a normal distribution around some mean between 2 and 5.

# Density plots for genes

The code below generates density plots for the expression levels of randomly selected genes. This helps visualize the distribution of RNA expression for each gene across the two cell types (CD4+T and TREG).

```{r}
# Randomly select 5 genes for visualization
set.seed(Sys.time())  # Set a random seed for reproducibility
selected_genes <- sample(colnames(RNAdata[,-1]), 5)

# Loop through the selected genes and create density plots
for (gene in selected_genes) {
  p <- ggplot(RNAdata, aes(x = .data[[gene]], fill = label)) +  # Use full RNAdata with `label`
    geom_density(alpha = 0.5) +
    labs(title = paste("Density Plot for Gene:", gene), x = "Expression Level", y = "Density") +
    theme_minimal()
  
  print(p)  # Explicitly print each plot
}
```

The density plots for most genes reveal highly overlapping distributions for CD4+T and TREG cells, suggesting that many genes do not exhibit significant differential expression between the two cell types. This overlap implies that these genes may not be strong discriminators for classification and might be less relevant for building effective predictive models. Such observations underscore the importance of identifying key genes or features that show greater separation between the classes, as they are likely to contribute more meaningfully to the task of binary classification.

# Scatter plots

The scatter plots also helps us visualize the expression levels of randomly selected pairs of genes, comparing their distributions between the two cell types (CD4+T and TREG)

```{r}
# Randomly select 2 pairs of genes for scatter plots
set.seed(Sys.time())
selected_genes <- sample(colnames(RNAdata[,-1]), 4)

# Create scatter plots for the selected pairs
library(ggplot2)
ggplot(RNAdata, aes(x = .data[[selected_genes[1]]], y = .data[[selected_genes[2]]], color = label)) +
  geom_point(alpha = 0.7) +
  labs(title = paste("Scatter Plot:", selected_genes[1], "vs", selected_genes[2]), 
       x = selected_genes[1], y = selected_genes[2]) +
  theme_minimal()

ggplot(RNAdata, aes(x = .data[[selected_genes[3]]], y = .data[[selected_genes[4]]], color = label)) +
  geom_point(alpha = 0.7) +
  labs(title = paste("Scatter Plot:", selected_genes[3], "vs", selected_genes[4]), 
       x = selected_genes[3], y = selected_genes[4]) +
  theme_minimal()

```

The significant overlap in distributions between CD4+T and TREG cells indicates that these gene pairs may not be highly informative for distinguishing the two classes. This highlights the importance of identifying gene pairs or combinations with stronger class separation for effective feature selection and classification.

### Sparsity of Data

RNA data is often sparse, i.e. has a significant number of null-values - our histogram plots above already indicate this. We calculate the sparsity of our dataframe by dividing the number of zero entries by the total number of entries.

```{r}
sum(RNAdata == 0)/(dim(RNAdata)[1]*dim(RNAdata)[2])
```

We get that 66.17% of our entries are nullvalues, meaning that our data is very sparse and thus less variant, as most entries are repeated.

## T1.2 Training and evaluation of classifiers

### Split data into Train and Test data

```{r}
# Loading necessary libraries
library(MASS)
library(caret)
library(pROC)
library(klaR)
library(randomForest)
library(class)
library(e1071)
library(gbm)

# Set seed for reproducibility
set.seed(123)  

# Create training (80%) and testing (20%) sets
sample_index <- createDataPartition(RNAdata$label, p = 0.8, list = FALSE)
train_data1 <- RNAdata[sample_index, ]
test_data1 <- RNAdata[-sample_index, ]
```

### Linear Discriminant Analysis (LDA)

```{r}
# Train the LDA model
lda_model <- lda(label ~ ., data = train_data1)
```

```{r}
# Make predictions on the test data
lda_predictions <- predict(lda_model, test_data1[,-1])
lda_predicted_labels <- lda_predictions$class
```

```{r}
# Confusion matrix
lda_conf_matrix <- confusionMatrix(lda_predicted_labels, as.factor(test_data1$label))
lda_conf_matrix
```

```{r}
# Extracting specific metrics
lda_accuracy <- lda_conf_matrix$overall['Accuracy']
lda_balanced_accuracy <- lda_conf_matrix$byClass['Balanced Accuracy']
lda_f1_score <- lda_conf_matrix$byClass['F1']
```

```{r}
cat("LDA Accuracy:", lda_accuracy, "\n")
cat("LDA Balanced Accuracy:", lda_balanced_accuracy, "\n")
cat("LDA F1 Score:", lda_f1_score, "\n")
```

```{r}
# Convert labels to numeric for ROC curve calculation
lda_probs <- lda_predictions$posterior[, 2]

# Create ROC curve
lda_roc_curve <- roc(as.factor(test_data1$label), lda_probs)
plot(lda_roc_curve, col = "pink", main = "ROC Curve for LDA")
lda_auc_value <- auc(lda_roc_curve)
cat("AUC:", lda_auc_value, "\n")
```

```{r}
cat("LDA Model Evaluation Summary:\n")
cat("Accuracy:", round(lda_accuracy, 3), "\n")
cat("Balanced Accuracy:", round(lda_balanced_accuracy, 3), "\n")
cat("F1 Score:", round(lda_f1_score, 3), "\n")
cat("AUC:", round(lda_auc_value, 3), "\n")
```

### Logistic Classifier

```{r}
train_data_log <- train_data1
test_data_log <- test_data1

train_data_log$label <- ifelse(train_data_log$label == "TREG", 1, 0)
test_data_log$label <- ifelse(test_data_log$label == "TREG", 1, 0)

# Train a logistic regression model
logistic_model <- glm(label ~ ., data = train_data_log, family = binomial)

# Print the model summary
#summary(logistic_model)
```

```{r}
# Predict probabilities on the test set
logistic_predictions <- predict(logistic_model, test_data_log[, -1], type = "response")

# Convert probabilities to class labels (threshold = 0.5)
logistic_predicted_labels <- ifelse(logistic_predictions > 0.5, 2, 1)
```

```{r}
# Calculate test error
logistic_test_error <- mean(logistic_predicted_labels != test_data_log$label)

# Print the test error
cat("Test Error (Logistic):", round(logistic_test_error, 3), "\n")
```

```{r}
# Confusion matrix
logistic_conf_matrix <- confusionMatrix(as.factor(logistic_predicted_labels), as.factor(test_data_log$label))
print(logistic_conf_matrix)
```

```{r}
# Extract metrics from the confusion matrix
accuracy_logistic <- logistic_conf_matrix$overall['Accuracy']
balanced_accuracy_logistic <- logistic_conf_matrix$byClass['Balanced Accuracy']
f1_score_logistic <- logistic_conf_matrix$byClass['F1']

# Print the metrics
cat("Accuracy (Logistic):", accuracy_logistic, "\n")
cat("Balanced Accuracy (Logistic):", balanced_accuracy_logistic, "\n")
cat("F1 Score (Logistic):", f1_score_logistic, "\n")
```

```{r}
# Calculate and plot the ROC curve
roc_curve_logistic <- roc(test_data_log$label, logistic_predictions)  # Use probabilities for ROC
plot(roc_curve_logistic, main = "ROC Curve for Logistic Regression", col='red')
auc_logistic <- auc(roc_curve_logistic)
cat("AUC:", auc_logistic, "\n")
```

Logistic Regression Model Evaluation Summary:

```{r}
cat("Logistic Regression Model Evaluation Summary:\n")
cat("Accuracy:", round(accuracy_logistic, 3), "\n")
cat("Balanced Accuracy:", round(balanced_accuracy_logistic, 3), "\n")
cat("F1 Score:", round(f1_score_logistic, 3), "\n")
cat("AUC:", round(auc_logistic, 3), "\n")
```

### Quadratic Discriminant Analysis (QDA)

```{r}

test_data_factor <- test_data1
train_data_factor <- train_data1

test_data_factor$label <- factor(test_data_factor$label)
train_data_factor$label <- factor(train_data_factor$label)

# Train a QDA model (using rda from klaR library to account for the high-dimensional data by introducing a small regularization factor to the covariance matrices, making them invertible even with the few samples available)
qda_model <- rda(label ~ ., data = train_data_factor, gamma = 0.01, lambda = 0.5)

# Print the model summary
summary(qda_model)
```

```{r}
# Predict the label of the test data
qda_predictions <- predict(qda_model, test_data_factor[,-1])  # Exclude label column
qda_predicted_labels <- qda_predictions$class
```

```{r}
# Confusion matrix
qda_conf_matrix <- confusionMatrix(qda_predicted_labels, test_data_factor$label)
qda_conf_matrix
```

```{r}
# Extract metrics from the confusion matrix
qda_accuracy <- qda_conf_matrix$overall['Accuracy']
qda_balanced_accuracy <- qda_conf_matrix$byClass['Balanced Accuracy']
qda_f1_score <- qda_conf_matrix$byClass['F1']

# Print the metrics
cat("Accuracy (QDA):", qda_accuracy, "\n")
cat("Balanced Accuracy (QDA):", qda_balanced_accuracy, "\n")
cat("F1 Score (QDA):", qda_f1_score, "\n")
```

```{r}
# Calculate and plot the ROC curve
roc_curve_qda <- roc(test_data1$label, qda_predictions)  # Use probabilities for ROC
plot(roc_curve_qda, main = "ROC Curve for QDA", col='red')
auc_qda <- auc(roc_curve_qda)
cat("AUC:", auc_qda, "\n")
```

```{r}
cat("QDA Model Evaluation Summary:\n")
cat("Accuracy:", round(qda_accuracy, 3), "\n")
cat("Balanced Accuracy:", round(qda_balanced_accuracy, 3), "\n")
cat("F1 Score:", round(qda_f1_score, 3), "\n")
cat("AUC:", round(qda_auc_value, 3), "\n")
```

### Random Forest


```{r}
# Train Random Forest with mtry = sqrt(number of predictors)
p <- ncol(train_data1) - 1  # Number of predictors (excluding the label column)
train_data1$label <- as.factor(train_data1$label)
rf_model <- randomForest(label ~ ., data = train_data1, mtry = round(sqrt(p)), ntree = 500, importance = TRUE)

# Print the model summary
print(rf_model)
```

```{r}
# Predictions on the test data
rf_predictions <- predict(rf_model, test_data1)

# Evaluating the model with a confusion matrix
conf_matrix_rf <- confusionMatrix(rf_predictions, as.factor(test_data1$label))
print(conf_matrix_rf)4
```

```{r}
# Extracting specific performance metrics
accuracy_rf <- conf_matrix_rf$overall['Accuracy']
balanced_accuracy_rf <- conf_matrix_rf$byClass['Balanced Accuracy']
f1_score_rf <- conf_matrix_rf$byClass['F1']

cat("Accuracy:", accuracy_rf, "\n")
cat("Balanced Accuracy:", balanced_accuracy_rf, "\n")
cat("F1 Score:", f1_score_rf, "\n")
```

```{r}
# Getting predicted probabilities for "TREG" class
rf_probs <- predict(rf_model, test_data1, type = "prob")[, 2]
```

```{r}
# Create ROC curve
roc_curve_rf <- roc(test_data1$label, rf_probs)
plot(roc_curve_rf, col = "blue", main = "ROC Curve for Random Forest")
```

```{r}
#calculating AUC
auc_value_rf <- auc(roc_curve_rf)
cat("AUC:", auc_value_rf, "\n")
```

```{r}
cat("Random Forest Evaluation Summary:\n")
cat("Accuracy:", round(accuracy_rf, 3), "\n")
cat("Balanced Accuracy:", round(balanced_accuracy_rf, 3), "\n")
cat("F1 Score:", round(f1_score_rf, 3), "\n")
cat("AUC:", round(auc_value_rf, 3), "\n")
```

### K-NN

```{r}
# Create matrices for predictors (X)
train_X <- as.matrix(train_data1[, -1])  # Exclude the label column
test_X <- as.matrix(test_data1[, -1])

# Create vectors for labels (Y)
train_Y <- train_data1$label
test_Y <- test_data1$label
```

```{r}
# Setting k = 5
k <- 5

# Perform k-NN classification
knn_predictions <- knn(train_X, test_X, train_Y, k = k)
```

```{r}
# Confusion matrix
conf_matrix_knn <- confusionMatrix(knn_predictions, as.factor(test_Y))
print(conf_matrix_knn)

# Extract the confusion matrix as a table
confusion_table <- conf_matrix_knn$table
print(confusion_table)

#Calculating Accuracy
accuracy_knn <- sum(diag(confusion_table)) / sum(confusion_table)
cat("Accuracy:", accuracy_knn, "\n")

```

```{r}
precision_knn <- conf_matrix_knn$byClass["Precision"]
recall_knn <- conf_matrix_knn$byClass["Recall"]
f1_score_knn <- 2 * ((precision_knn * recall_knn) / (precision_knn + recall_knn))
cat("F1 Score:", f1_score_knn, "\n")
```

```{r}
sensitivity_knn <- conf_matrix_knn$byClass["Sensitivity"]
specificity_knn <- conf_matrix_knn$byClass["Specificity"]
balanced_accuracy_knn <- (sensitivity_knn + specificity_knn) / 2
cat("Balanced Accuracy:", balanced_accuracy_knn, "\n")
```

```{r}
# Calculate pseudo-probabilities for AUC
k <- 5  #The same k as used above
knn_probabilities <- attr(knn_predictions, "prob")
knn_probabilities <- ifelse(knn_predictions == "TREG", knn_probabilities, 1 - knn_probabilities)
summary(knn_probabilities)  # Ensure values range between 0 and 1
```

```{r}
# Check levels of the test_Y factor
levels(test_Y)
table(test_Y)
```

```{r}
test_Y_numeric <- ifelse(test_Y == "TREG", 1, 0)  # Assuming "TREG" is the positive class
table(test_Y_numeric)
```

```{r}
# Calculate AUC
roc_curve_knn <- roc(test_Y_numeric, knn_probabilities)
auc_value_knn <- auc(roc_curve_knn)
cat("AUC:", auc_value_knn, "\n")
```

```{r}
# Plot ROC curve
plot(roc_curve_knn, col = "blue", main = "ROC Curve for k-NN")
```

```{r}
cat("Knn Classifiers Summary:\n")
cat("Accuracy:", round(accuracy_knn, 3), "\n")
cat("Balanced Accuracy:", round(balanced_accuracy_knn, 3), "\n")
cat("F1 Score:", round(f1_score_knn, 3), "\n")
cat("AUC:", round(auc_value_knn, 3), "\n")
```

### Support Vector Machine (SVM)

```{r}
test_data_factor <- test_data1
train_data_factor <- train_data1

test_data_factor$label <- factor(test_data_factor$label)
train_data_factor$label <- factor(train_data_factor$label)

#Model
svm_model <- svm(label ~ ., data = train_data_factor, kernel = "linear", probability = TRUE)
svm_predictions <- predict(svm_model, test_data_factor[,-1], probability = TRUE)
svm_pred_prob <- attr(svm_predictions, "probabilities")[, 2]
```

```{r}
# Accuracy
svm_accuracy <- mean(svm_predictions == test_data_factor$label)

# Confusion matrix
svm_conf_matrix <- confusionMatrix(svm_predictions, test_data_factor$label)

# Balanced accuracy
svm_balanced_accuracy <- svm_conf_matrix$byClass['Balanced Accuracy']

# F1 Score
svm_f1 <- svm_conf_matrix$byClass['F1']

# ROC Curve & AUC
svm_roc_curve <- roc(test_data_factor$label, svm_pred_prob)
plot(svm_roc_curve, main = "ROC Curve", col = "blue", lwd = 2)
svm_auc <- auc(svm_roc_curve)
```
```{r}
cat("Support Vector Machine Model Evaluation Summary:\n")
cat("Accuracy:", round(svm_accuracy, 3), "\n")
cat("Balanced Accuracy:", round(svm_balanced_accuracy, 3), "\n")
cat("F1 Score:", round(svm_f1, 3), "\n")
cat("AUC:", round(svm_auc, 3), "\n")
```

### Gradient Boosting Decision Tree (GBDT)


```{r}
test_data_gbdt <- test_data1
train_data_gbdt <- train_data1

test_data_gbdt$label <- as.numeric(as.factor(test_data_gbdt$label)) - 1
train_data_gbdt$label <- as.numeric(as.factor(train_data_gbdt$label)) - 1

#Model
gbm_model <- gbm(label ~ ., data = train_data_gbdt, 
                 distribution = "bernoulli", 
                 n.trees = 150, 
                 interaction.depth = 5, 
                 cv.folds = 5,
                 shrinkage = 0.1,
                 n.minobsinnode = 20,
                 verbose = FALSE)

best_trees <- gbm.perf(gbm_model, method = "cv")

gbdt_predictions <- predict(gbm_model, test_data_gbdt[,-1], n.trees = best_trees, type = "response")

gbdt_predictions_class <- ifelse(gbdt_predictions > 0.5, 1, 0)
```

```{r}
# Accuracy
gbdt_accuracy <- mean(gbdt_predictions_class == test_data_gbdt$label)

# Confusion Matrix
gbdt_conf_matrix <- confusionMatrix(as.factor(gbdt_predictions_class), as.factor(test_data_gbdt$label))

# Balanced Accuracy
gbdt_balanced_accuracy <- mean(gbdt_conf_matrix$byClass["Sensitivity"], gbdt_conf_matrix$byClass["Specificity"])

# F1 Score
gbdt_f1_score <- 2 * (gbdt_conf_matrix$byClass["Precision"] * gbdt_conf_matrix$byClass["Recall"]) / 
  (gbdt_conf_matrix$byClass["Precision"] + gbdt_conf_matrix$byClass["Recall"])

# AUC 
gbdt_roc_curve <- roc(test_data_gbdt$label, gbdt_predictions)
gbdt_auc_value <- auc(gbdt_roc_curve)

# ROC Curve
plot.roc(gbdt_roc_curve, main = "GBDT ROC Curve", col = "blue", lwd = 2)

```
```{r}
cat("Gradient Boosting Decision Trees Model Evaluation Summary:\n")
cat("Accuracy:", round(gbdt_accuracy, 3), "\n")
cat("Balanced Accuracy:", round(gbdt_balanced_accuracy, 3), "\n")
cat("F1 Score:", round(gbdt_f1_score, 3), "\n")
cat("AUC:", round(gbdt_auc_value, 3), "\n")
```




### PCA with 10 components ###

Split the data:
```{r}
# Perform PCA on the training data (excluding the label column)
pca <- prcomp(train_data1[, -1], scale. = TRUE)

# Keep only the first 10 principal components
train_pca <- data.frame(label = train_data1$label, pca$x[, 1:10])
test_pca <- data.frame(label = test_data1$label, predict(pca, test_data1[, -1])[, 1:10])
```


### LDA with PCA 

```{r}
# Train the LDA model on the PCA-transformed training data
lda_pca_model <- lda(label ~ ., data = train_pca)

# Print the LDA model summary
summary(lda_pca_model)
```


```{r}
# Make predictions on the PCA-transformed test data
lda_pca_predictions <- predict(lda_pca_model, test_pca)

# Confusion matrix

conf_matrix_pca <- confusionMatrix(as.factor(lda_pca_predictions$class), as.factor(test_pca$label))
print(conf_matrix_pca)

# Calculate test error
lda_pca_test_error <- mean(lda_pca_predictions$class != test_pca$label)
cat("LDA PCA Test Error:", round(lda_pca_test_error, 3), "\n")

```


```{r}
# Extract metrics from the confusion matrix
accuracy_lda_pca <- conf_matrix_pca$overall['Accuracy']
balanced_accuracy_lda_pca <- conf_matrix_pca$byClass['Balanced Accuracy']
f1_score_lda_pca <- conf_matrix_pca$byClass['F1']

# Print the metrics
cat("Accuracy (LDA with PCA):", accuracy_lda_pca, "\n")
cat("Balanced Accuracy (LDA with PCA):", balanced_accuracy_lda_pca, "\n")
cat("F1 Score (LDA with PCA):", f1_score_lda_pca, "\n")
```



```{r}
# Calculate and plot the ROC curve
roc_curve_pca <- roc(test_pca$label, as.numeric(lda_pca_predictions$class), levels = c("TREG", "CD4+T"))
plot(roc_curve_pca, main = "ROC Curve for LDA with PCA", col='red')
auc_pca <- auc(roc_curve_pca)
cat("AUC (LDA with PCA):", round(auc_pca, 3), "\n")
```

```{r}
cat("LDA with PCA Model Evaluation Summary:\n")
cat("Accuracy:", round(accuracy_lda_pca, 3), "\n")
cat("Balanced Accuracy:", round(balanced_accuracy_lda_pca, 3), "\n")
cat("F1 Score:", round(f1_score_lda_pca, 3), "\n")
cat("AUC:", round(auc_pca, 3), "\n")
```



### Logistic classifier with PCA

```{r}
# Create binary-labeled datasets for logistic regression
train_pca_log <- train_pca
test_pca_log <- test_pca

# Convert 'label' to binary numeric variable
train_pca_log$label <- ifelse(train_pca_log$label == "TREG", 1, 0)
test_pca_log$label <- ifelse(test_pca_log$label == "TREG", 1, 0)

```


```{r}
# Train logistic regression on PCA-transformed data
logistic_pca_model <- glm(label ~ ., data = train_pca_log, family = binomial)

# Print the model summary
summary(logistic_pca_model)

```

```{r}
# Predict probabilities on the PCA-transformed test data
logistic_pca_predictions <- predict(logistic_pca_model, newdata = test_pca_log, type = "response")

# Convert probabilities to class labels (threshold = 0.5)
logistic_pca_predicted_labels <- ifelse(logistic_pca_predictions > 0.5, 1, 0)

# Calculate test error
logistic_pca_test_error <- mean(logistic_pca_predicted_labels != test_pca_log$label)
cat("Test Error (Logistic with PCA):", round(logistic_pca_test_error, 3), "\n")

```


```{r}
# Confusion matrix

conf_matrix_pca <- confusionMatrix(as.factor(logistic_pca_predicted_labels), as.factor(test_pca_log$label))
print(conf_matrix_pca)

# Extract metrics from the confusion matrix
accuracy_logistic_pca <- conf_matrix_pca$overall['Accuracy']
balanced_accuracy_logistic_pca <- conf_matrix_pca$byClass['Balanced Accuracy']
f1_score_logistic_pca <- conf_matrix_pca$byClass['F1']

# Print the metrics
cat("Accuracy (Logistic with PCA):", accuracy_logistic_pca, "\n")
cat("Balanced Accuracy (Logistic with PCA):", balanced_accuracy_logistic_pca, "\n")
cat("F1 Score (Logistic with PCA):", f1_score_logistic_pca, "\n")


```


```{r}
# Calculate and plot the ROC curve

roc_curve_logistic_pca <- roc(test_pca_log$label, logistic_pca_predictions)
plot(roc_curve_logistic_pca, main = "ROC Curve for Logistic Regression with PCA", col = 'red')
auc_logistic_pca <- auc(roc_curve_logistic_pca)
cat("AUC (Logistic with PCA):", auc_logistic_pca, "\n")

```

```{r}
cat("Logistic Regression with PCA Model Evaluation Summary:\n")
cat("Accuracy:", round(accuracy_logistic_pca, 3), "\n")
cat("Balanced Accuracy:", round(balanced_accuracy_logistic_pca, 3), "\n")
cat("F1 Score:", round(f1_score_logistic_pca, 3), "\n")
cat("AUC:", round(auc_logistic_pca, 3), "\n")

```

### QDA with PCA

```{r}
# Train a QDA model on the PCA-transformed data
install.packages("questionr")
library(questionr)
library(klaR)
qda_pca_model <- rda(label ~ ., data = train_pca, gamma = 0.01, lambda = 0.5)

# Print the model summary
summary(qda_pca_model)

```

```{r}
# Predict the label of the test data
qda_pca_predictions <- predict(qda_pca_model, test_pca[,-1])  # Exclude label column
qda_pca_predicted_labels <- qda_pca_predictions$class

# Confusion matrix
qda_pca_conf_matrix <- confusionMatrix(as.factor(qda_pca_predicted_labels), as.factor(test_pca$label))
qda_pca_conf_matrix

```


```{r}
# Extract metrics from the confusion matrix
qda_pca_accuracy <- qda_pca_conf_matrix$overall['Accuracy']
qda_pca_balanced_accuracy <- qda_pca_conf_matrix$byClass['Balanced Accuracy']
qda_pca_f1_score <- qda_pca_conf_matrix$byClass['F1']

# Print the metrics
cat("Accuracy (QDA with PCA):", qda_pca_accuracy, "\n")
cat("Balanced Accuracy (QDA with PCA):", qda_pca_balanced_accuracy, "\n")
cat("F1 Score (QDA with PCA):", qda_pca_f1_score, "\n")

```


```{r}
# Calculate the ROC curve
qda_pca_roc_curve <- roc(test_pca$label, qda_pca_predictions$posterior[, "TREG"], levels = c("TREG", "CD4+T"))
plot(qda_pca_roc_curve, main = "ROC Curve for QDA with PCA", col = 'red')
qda_pca_auc <- auc(qda_pca_roc_curve)
cat("AUC (QDA with PCA):", round(qda_pca_auc, 3), "\n")


```

```{r}
cat("QDA with PCA Model Evaluation Summary:\n")
cat("Accuracy:", round(qda_pca_accuracy, 3), "\n")
cat("Balanced Accuracy:", round(qda_pca_balanced_accuracy, 3), "\n")
cat("F1 Score:", round(qda_pca_f1_score, 3), "\n")
cat("AUC:", round(qda_pca_auc, 3), "\n")

```


### Random Forest with PCA 

```{r}
install.packages("randomForest")
library(randomForest)

# Ensure the label column is a factor
train_pca$label <- as.factor(train_pca$label)
test_pca$label <- as.factor(test_pca$label)

# Train Random Forest on PCA-transformed data
p <- ncol(train_pca) - 1  # Number of predictors (10 components)
rf_pca_model <- randomForest(label ~ ., data = train_pca, mtry = round(sqrt(p)), ntree = 500, importance = TRUE)

# Print the model summary
print(rf_pca_model)

```


```{r}
# Predictions on the test data
rf_pca_predictions <- predict(rf_pca_model, test_pca)

# Confusion matrix
conf_matrix_rf_pca <- confusionMatrix(rf_pca_predictions, test_pca$label)
print(conf_matrix_rf_pca)

```

```{r}
# Extract specific performance metrics
accuracy_rf_pca <- conf_matrix_rf_pca$overall['Accuracy']
balanced_accuracy_rf_pca <- conf_matrix_rf_pca$byClass['Balanced Accuracy']
f1_score_rf_pca <- conf_matrix_rf_pca$byClass['F1']

# Print the metrics
cat("Accuracy (Random Forest with PCA):", round(accuracy_rf_pca, 3), "\n")
cat("Balanced Accuracy (Random Forest with PCA):", round(balanced_accuracy_rf_pca, 3), "\n")
cat("F1 Score (Random Forest with PCA):", round(f1_score_rf_pca, 3), "\n")

```

```{r}
# Get predicted probabilities for "TREG" class
rf_pca_probs <- predict(rf_pca_model, test_pca, type = "prob")[, "TREG"]

# Create ROC curve
roc_curve_rf_pca <- roc(test_pca$label, rf_pca_probs, levels = c("TREG", "CD4+T"))
plot(roc_curve_rf_pca, col = "red", main = "ROC Curve for Random Forest with PCA")

# Calculate AUC
auc_value_rf_pca <- auc(roc_curve_rf_pca)
cat("AUC (Random Forest with PCA):", round(auc_value_rf_pca, 3), "\n")

```

```{r}
cat("Random Forest with PCA Evaluation Summary:\n")
cat("Accuracy:", round(accuracy_rf_pca, 3), "\n")
cat("Balanced Accuracy:", round(balanced_accuracy_rf_pca, 3), "\n")
cat("F1 Score:", round(f1_score_rf_pca, 3), "\n")
cat("AUC:", round(auc_value_rf_pca, 3), "\n")

```


### K-NN with PCA

```{r}
# Create matrices for predictors (X)
train_X_pca <- as.matrix(train_pca[, -1])  # Exclude the label column
test_X_pca <- as.matrix(test_pca[, -1])

# Create vectors for labels (Y)
train_Y_pca <- train_pca$label
test_Y_pca <- test_pca$label

```


```{r}
# Train k-NN model with probabilities using caret's knn3
k <- 5  # Set the value of k
knn_model_pca <- knn3(train_X_pca, train_Y_pca, k = k)

# Predict probabilities and classes for the test set
knn_probabilities_pca <- predict(knn_model_pca, test_X_pca, type = "prob")
knn_pca_predictions <- predict(knn_model_pca, test_X_pca, type = "class")

```

# Reason we are using knn3 here:
The knn function from the class package works for simple k-NN because it directly predicts class labels based on the nearest neighbors. However, when we apply PCA, we need class probabilities to compute metrics like AUC and plot the ROC curve. The knn function doesn't support probability predictions, which is why it doesn't work for k-NN with PCA when AUC is required.

In contrast, knn3 from the caret package supports both class predictions and probability outputs, making it suitable for PCA-based k-NN with advanced metrics like AUC.


```{r}
# Confusion matrix
conf_matrix_knn_pca <- confusionMatrix(knn_pca_predictions, as.factor(test_Y_pca))
print(conf_matrix_knn_pca)

```

```{r}
# Extract specific metrics
accuracy_knn_pca <- conf_matrix_knn_pca$overall['Accuracy']
precision_knn_pca <- conf_matrix_knn_pca$byClass['Precision']
recall_knn_pca <- conf_matrix_knn_pca$byClass['Recall']
sensitivity_knn_pca <- conf_matrix_knn_pca$byClass['Sensitivity']
specificity_knn_pca <- conf_matrix_knn_pca$byClass['Specificity']

# Calculate F1 Score and Balanced Accuracy
f1_score_knn_pca <- 2 * ((precision_knn_pca * recall_knn_pca) / (precision_knn_pca + recall_knn_pca))
balanced_accuracy_knn_pca <- (sensitivity_knn_pca + specificity_knn_pca) / 2

# Print metrics
cat("Accuracy (k-NN with PCA):", round(accuracy_knn_pca, 3), "\n")
cat("Balanced Accuracy (k-NN with PCA):", round(balanced_accuracy_knn_pca, 3), "\n")
cat("F1 Score (k-NN with PCA):", round(f1_score_knn_pca, 3), "\n")

```


```{r}
# Convert labels to numeric for ROC calculation
test_Y_numeric_pca <- ifelse(test_Y_pca == "TREG", 1, 0)

# Extract probabilities for the positive class ("TREG")
knn_probs_treg <- knn_probabilities_pca[, "TREG"]



# Calculate and plot the ROC curve
roc_curve_knn_pca <- roc(test_Y_numeric_pca, knn_probs_treg)
auc_value_knn_pca <- auc(roc_curve_knn_pca)

# Plot ROC curve
plot(roc_curve_knn_pca, col = "red", main = "ROC Curve for k-NN with PCA")
cat("AUC (k-NN with PCA):", round(auc_value_knn_pca, 3), "\n")


```

```{r}
cat("k-NN with PCA Classifier Summary:\n")
cat("Accuracy:", round(accuracy_knn_pca, 3), "\n")
cat("Balanced Accuracy:", round(balanced_accuracy_knn_pca, 3), "\n")
cat("F1 Score:", round(f1_score_knn_pca, 3), "\n")
cat("AUC:", round(auc_value_knn_pca, 3), "\n")

```


### SVM with QDA 

```{r}
# Train the SVM model on PCA-transformed data
svm_model_pca <- svm(label ~ ., data = train_pca, probability = TRUE)

# Print the SVM model summary
summary(svm_model_pca)
```

```{r}
# Predict class labels
svm_pca_predictions <- predict(svm_model_pca, test_pca, probability = TRUE)

# Predict probabilities
svm_pca_probs <- attr(predict(svm_model_pca, test_pca, probability = TRUE), "probabilities")

```

```{r}
# Confusion matrix
conf_matrix_svm_pca <- confusionMatrix(svm_pca_predictions, as.factor(test_pca$label))
print(conf_matrix_svm_pca)
```

```{r}
# Extract specific metrics
accuracy_svm_pca <- conf_matrix_svm_pca$overall['Accuracy']
precision_svm_pca <- conf_matrix_svm_pca$byClass['Precision']
recall_svm_pca <- conf_matrix_svm_pca$byClass['Recall']
sensitivity_svm_pca <- conf_matrix_svm_pca$byClass['Sensitivity']
specificity_svm_pca <- conf_matrix_svm_pca$byClass['Specificity']

# Calculate F1 Score and Balanced Accuracy
f1_score_svm_pca <- 2 * ((precision_svm_pca * recall_svm_pca) / (precision_svm_pca + recall_svm_pca))
balanced_accuracy_svm_pca <- (sensitivity_svm_pca + specificity_svm_pca) / 2

# Print metrics
cat("Accuracy (SVM with PCA):", round(accuracy_svm_pca, 3), "\n")
cat("Balanced Accuracy (SVM with PCA):", round(balanced_accuracy_svm_pca, 3), "\n")
cat("F1 Score (SVM with PCA):", round(f1_score_svm_pca, 3), "\n")

```


```{r}
# Convert labels to numeric for ROC calculation
test_Y_numeric_pca <- ifelse(test_pca$label == "TREG", 1, 0)

# Extract probabilities for the positive class ("TREG")
svm_probs_treg <- svm_pca_probs[, "TREG"]

# Calculate and plot the ROC curve
roc_curve_svm_pca <- roc(test_Y_numeric_pca, svm_probs_treg)
auc_value_svm_pca <- auc(roc_curve_svm_pca)

# Plot ROC curve
plot(roc_curve_svm_pca, col = "red", main = "ROC Curve for SVM with PCA")
cat("AUC (SVM with PCA):", round(auc_value_svm_pca, 3), "\n")
```


```{r}
cat("SVM with PCA Classifier Summary:\n")
cat("Accuracy:", round(accuracy_svm_pca, 3), "\n")
cat("Balanced Accuracy:", round(balanced_accuracy_svm_pca, 3), "\n")
cat("F1 Score:", round(f1_score_svm_pca, 3), "\n")
cat("AUC:", round(auc_value_svm_pca, 3), "\n")

```


### GBDT with PCA 

```{r}

# Convert labels to numeric (required for GBDT)
train_pca$label <- as.numeric(as.factor(train_pca$label)) - 1
test_pca$label <- as.numeric(as.factor(test_pca$label)) - 1

library(gbm)

# Train GBDT model on PCA-transformed data
gbm_pca_model <- gbm(label ~ ., data = train_pca,
                     distribution = "bernoulli",
                     n.trees = 150,
                     interaction.depth = 5,
                     cv.folds = 5,
                     shrinkage = 0.1,
                     n.minobsinnode = 20,
                     verbose = FALSE)

# Determine the optimal number of trees
best_trees_pca <- gbm.perf(gbm_pca_model, method = "cv")

```

```{r}
# Predict probabilities and class labels
gbdt_pca_predictions <- predict(gbm_pca_model, test_pca[,-1], n.trees = best_trees_pca, type = "response")
gbdt_pca_predictions_class <- ifelse(gbdt_pca_predictions > 0.5, 1, 0)

```

```{r}
# Confusion matrix
gbdt_pca_conf_matrix <- confusionMatrix(as.factor(gbdt_pca_predictions_class), as.factor(test_pca$label))
print(gbdt_pca_conf_matrix)

# Extract metrics
gbdt_pca_accuracy <- mean(gbdt_pca_predictions_class == test_pca$label)
gbdt_pca_balanced_accuracy <- mean(c(gbdt_pca_conf_matrix$byClass["Sensitivity"], gbdt_pca_conf_matrix$byClass["Specificity"]))
gbdt_pca_f1_score <- 2 * (gbdt_pca_conf_matrix$byClass["Precision"] * gbdt_pca_conf_matrix$byClass["Recall"]) / 
  (gbdt_pca_conf_matrix$byClass["Precision"] + gbdt_pca_conf_matrix$byClass["Recall"])

# Print metrics
cat("Accuracy (GBDT with PCA):", round(gbdt_pca_accuracy, 3), "\n")
cat("Balanced Accuracy (GBDT with PCA):", round(gbdt_pca_balanced_accuracy, 3), "\n")
cat("F1 Score (GBDT with PCA):", round(gbdt_pca_f1_score, 3), "\n")
```


```{r}
# Calculate and plot the ROC curve
gbdt_pca_roc_curve <- roc(test_pca$label, gbdt_pca_predictions)
gbdt_pca_auc_value <- auc(gbdt_pca_roc_curve)

# Plot ROC curve
plot.roc(gbdt_pca_roc_curve, main = "GBDT ROC Curve with PCA", col = "red", lwd = 2)
cat("AUC (GBDT with PCA):", round(gbdt_pca_auc_value, 3), "\n")
```


```{r}
cat("GBDT with PCA Classifier Summary:\n")
cat("Accuracy:", round(gbdt_pca_accuracy, 3), "\n")
cat("Balanced Accuracy:", round(gbdt_pca_balanced_accuracy, 3), "\n")
cat("F1 Score:", round(gbdt_pca_f1_score, 3), "\n")
cat("AUC:", round(gbdt_pca_auc_value, 3), "\n")

```



## T1.3 Training and evaluation of classifiers
